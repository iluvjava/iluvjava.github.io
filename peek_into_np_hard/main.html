<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">
    <title>Peek Into Np-Hard</title>
    
    <!--For Bootstrap style-->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
     integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!--Bootstrap end-->
    
    <!--MyStyle-->
    <link rel="stylesheet" href="mystyle.css">
    <!--End-->

    <!--Nothing here yet. -->
    <!--For Plottind data and Adpating Webpage to mobile devices-->
    <script src="client_scripts_general.js"></script>
    <!-- End -->

    <!--Plotly-->
    <script src="https://cdn.plot.ly/plotly-1.2.0.min.js"></script> <!--FIX VERSION, new UPDATE has problem-->
    <!--END-->

    <!--JS hilight for the clock blocks in the web: -->
    <link rel="stylesheet"
      href="../assets/client_scripts/syntax_highlight_pack/styles/a11y-light.css">
    <script src="../assets/client_scripts/syntax_highlight_pack/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <!--END-->

  </head>
  <body>
    <nav class="navbar sticky-top">
      <div class="container-fluid">
        <div class="navbar-header">
          <a class="navbar-brand" href="../index.html">My WebPages</a>
        </div>
        <botton>
            <span class="nav-hamburger-menu-icon hide"><img src="../assets/menu_hamburger.png" alt="menu"></span> 
        </botton>
        <div class="collapse navbar-collapse">
          <ul>
            <li><a href="#intro">Introduction</a></li>
            <li><a href="#dp">Dynamic Programming</a></li>
            <li><a href="#approx">Approximation Algorithms</a></li>
            <li><a href="#bb">Branch and Bound algorithms</a></li>
            <li><a href="#lp">Knapsack but Linear Programming</a></li>
            <li>
              <a target="_blank" href="https://github.com/iluvjava/Silly_Python_Stuff/tree/master/knapsack">
                Github Repo for This Project
              </a>
            </li>
            <li><a href="#bench">Performance and Benchmark</a></li>
            <li><a href="#epilogue">Epilogue</a></li>
          </ul>
        </div>
      </div>
    </nav>
    <div id="side-bar">
      <ul>
        <li><a href="#intro">Introduction</a></li>
        <li><a href="#dp">Dynamic Programming</a></li>
        <li><a href="#approx">Approximation Algorithms</a></li>
        <li><a href="#bb">Branch and Bound algorithms</a></li>
        <li><a href="#lp">Knapsack but Linear Programming</a></li>
        <li><a target="_blank" href="https://github.com/iluvjava/Silly_Python_Stuff/tree/master/knapsack">
          Github Repo for This Project
        </a></li>
        <li><a href="#bench">Performance and Benchmark</a></li>
        <li><a href="#epilogue">Epilogue</a></li>
      </ul>
    </div>
    <div id="main-content">
      <section id="intro">
        <h1>
        Introduction
        </h1>
        <p>
          The Knapsack Problem is the best problem to learn about NP-Hardness 
          because it's simple, in terms of how the problem is defined. 
          <br>
          It's simple also because it's approximation algorithm is not NP-Hard.
        </p>
        <p>
          Formulation of the Knapsack problem: 
          \(\max\{\sum_{i=1}^n p_ix_i : \sum_{i=1}^n w_i x_i \leq b\wedge x\in\{0, 1\}^n \}\)
          and to eliminates edge cases, let's also define that: 
        </p>
        <ol>
          <li>
            For all item's profits: \(p_i \in \mathbb{R}_+\)
          </li>
          <li>
            For all item's weight: \(0 \leq w_i \leq b; w_i\in \mathbb{R}_+\)
          </li>
          <li>
            I am going to use index starting with 0 and ends with n-1 for all 
            parts after this: 
          </li>
        </ol>
      </section>
      <hr>
      <section id="dp">
        <h1>
          Dynamic Programming
        </h1>
        <p>
          Dynamic Programming is possible if one of the value, \(\vec{w}\) or 
          \(\vec{p}\) is a vector of integers. This makes things easy because 
          it allows the Dynamic Programming algorithm to optimize for subsets 
          of item sharing the same weights/profits. 
          <br>
          For the contrary, imagine the weights and profits of the item is random 
          points on the real number line, then it's impossible to have the 
          different subsets having the same profits/weights. 
          <br>
          There are 2 solution to solving the Knapsack problem: 
          <ol>
            <li>Primal: Maximizing the profits for a certain amount of weights.</li>
            <li>Dual: Minimizing the weights for a certain amount profits. </li>
          </ol>
          This is one of the applications of duality in Linear Programming. 
        </p>
        
        <h2>Primal</h2>
        <p>
          Inputs: \(\forall w_i : w_i \in \mathbb{Z}_+\), \(\forall p_i: p_i \in \mathbb{R}_+\)
          <br>
          Observe that, one of the vector can have numbers that are real. 
          <ul>
            <li>
              T[j, w] := the profits of items using sub array from 0 to j such 
              that it sums up to weight of exactly w. 
              <br>
              It's up to the reader to make of sense that: \(0 \leq j \leq n-1\),
              \(0\leq w \leq b\)
            </li>
            <li>
              The recurrence relation: 
              <ul>
                <li>T[j + 1, w] := max(T[j, w - \(w_j\)] + \(p_{j+1}\), T[j, w])</li>
                <li>If w - \(w_j\) < 0 then just ignore that case. </li>
              </ul>
            </li>
            <li>
              Base Cases: 
              <ul>
                <li>T[-1, \(\forall \)] := -inf</li>
                <li>T[-1, 0] := 0</li>
              </ul>
            </li>
            <li>
              Order for updating: column, by columns. 
            </li>
            <li>
              Optimization: 
              <ul>
                <li>
                  Only the previous column of T is used for each iteration, so that can be simplied by storing only previous column. 
                </li>
                <li>
                  We need to keep track of the solution using a dictionary. 
                </li>
              </ul>
            </li>
            <li>
              On the table, look for entry maximizes value of T[j, w]. 
            </li>
          </ul>    
          The complexity of the Primal exact algorithm is the size o the table: 
          \(\mathcal{O}(n*b)\)
        </p>
        <h2>Dual</h2>
        <p>
          The dual tries to minimize the weight of the solution for a fixed 
          profits. And it allows weights to be positive real, and the profits has to be integers.
          <br>
          Inputs: \(\forall p_i: p_i \in \mathbb{Z}_+\), \(\forall w_i: w_i \in \mathbb{R}_+\)
          <ul>
            <li>
              Definition of table: T[j, p]: The minimum of weights such that it gives exactly a profits of p. 
            </li>
            <li>
              Recurrence: 
              <ul>
                <li> T[j, p] := min(T[j - 1, p - \(p_j\)] + \(w_j\), T[j - 1, p])</li>
                <li>If p - \(p_j\) < 0, then just ignore that case. </li>
              </ul>
            </li>
            <li>
              Order of update: column by columns. 
            </li>
            <li>
              Base cases: 
              <ul>
                <li>T[-1, \(\forall\)] := \(+\inf\);</li>
                <li> T[-1, 0]:= 0 </li>
              </ul>
            </li>
          </ul>
          The dual formulation will have a table with this many rows: \(\sum_{i} p_i\);  but the upper bound for profits 
          can be reduced using approximation algorithm: Greedy Algorithm. 
        </p>
      </section>
      <hr>
      <section id="approx">
        <h1>Approximation Algorithms</h1>
        <p>
          We need approximation algorithms for the reasons that: 
          <ul>
            <li>
              The weights and profits of items can both be real numbers, one of the way to handle that is to use 
              an approximation algorithms. 
            </li>
            <li>
              It also provides an upperbound for the branch & bound algorith, which is invaluable.    
            </li>
          </ul>
        </p>
        <h2>Greedy Algorithm</h2>
        <p>
          The greedy algorithm ranks the items by its value (measure as \(p_i/w_i\)), and then it takes a fractional 
          amount the the last item item that cannot fit into the budget.
        </p>
        <p> 
          The greedy algorithm ranks the items by its value (measure as \(p_i/w_i\)), and then it takes a fractional 
          amount the the last item item that cannot fit into the budget. 
        </p>
        <br>
        <pre>
          Rank all items by its value. 
          while: There is remaining budget: 
            take the item if the budget allows
            else: 
              take a fractional amount of that item to exhaust all budget, then break;
        </pre>
        <p>
          If the greedy algorithm's solution is all integers, then it's the absolute optimal 
          (almost impossible, if this happens, and you made the Knapsack problem yourself, you must be drunk)
          <br>
          This algorithm is extremely fast, and it provides an upperbound for a given Knapsack problem.
        </p>
        <h3>Claim I: </h3>
        <p>
          There there doesn't exist any subset that have an objective value that is larger than the 
          objective value of Greedy Algorithm(This is obvious if we prove it with linear programming). 
          <br>
        </p>
        <h3>Claim II: </h3>
        <p>
          The slack on the optimal solution is less than the minimum item with minimum weight in the Greedy solution. 
          The optimal solution has an objective value that is at least 1/2 of the greedy algorithm. 
          <br>
          <h4>Proof:</h4>
          <p>
            \(\tilde{S}\): The integeral solution produced by the greedy algorithm; \(S^+\): The optimal integral, 
            solution. They are both a set of indices.
            <br>
            Let k denotes the slack variable: \[k := b - \sum_{i\in S^+} w_i\]
            <br>
            Given the optimality assumption of \(S^+\), we know that we cannot move any element in \(\tilde{S}\) to increase
            the objective value of \(S^+\), therefore:
            \[\min_{i\in\tilde{S}}\{w_i\} > k\]
            Now assume somehow, the item with minimum weight in \(\min_{i\in\tilde{S}}\{w_i\} > b/2\), then the solution 
            of greedy must contain that one solution (there is not enough budget for 2 items), then the aboslute optimal
            will have to be at most \(2\sum_{i\in\tilde{S^+}}p_i\), otherwise, \(S^+\) will contain an item has more 
            value with less weight, which is a contradiction to how greedy algorithm works.  
            <br><br>
            For the ther case, assume that the value \(\min_{i\in\tilde{S}}\{w_i\} \leq b/2\), then \(k &lt; b/2\), making 
            not enough room for the absolute optimal to make more than double the profits compare to the greedy algorithm.
          </p> 
        </p>
        <h2>Dual with Rounding</h2>
        <p>
          The algorithm round the profits to integers and then use the Dual DP(Dynamic Programming) to solve the problem,
          here is the algorithm: 
          <br>
          Define the multiplier as: \[m:= \frac{n}{\max_i\{p_i\}\epsilon}\] 
          where \(0 &lt; \epsilon &lt; 1\), then define \(p_i' := \lfloor{m*p_i}\rfloor\), as the rounded profits; then 
          redefined \(p_i := m*p_i\)
          <br>
          Then use the Dual DP to solve the problem. 
          <h3>Claim I: </h3>
          <p>
            This algorithm asserts a lower bound on the optimal solution, let \(p(S):= \sum_{i\in S}p_i\)
            (profits are not rounded, but rescaled), and let the solution found by the rounded profits be \(\tilde{S}\), and let the 
            absolute optimal solution be: \(S^*\), then: \(p'(\tilde{S}) = p'(S^*) \geq (1 -\epsilon)p(S^*)\). 
            <h4>Proof: </h4>
            <p>
              First, it's obviously true that: \(p'(\tilde{S}) = p'(S^*)\), because DP asserts the optimality for the inputs,
              hence there is no way the absolute optimal can have high profits in the rounded problem. 
              <br>
              Starts with this basic facts: 

              \[\lfloor p_i \rfloor + 1 \geq p_i\]
              \[\lfloor p_i \rfloor \geq p_i - 1\]
              Then we have: 
              \[p'(S^*) \geq p(S^*) - n\]
              Consider the quantity: \(n\)
              \[\frac{n}{\epsilon} = \max_{i}\{p_i\} \leq p(S^*)\]
              The \(p_i\) inside of the maximum operator is not scaled! 
              \(S^*\) non-empty by preconditions. 
              then we have: 
              \[p'(S^*) \geq p(S^*) - n \geq p(S^*) - \epsilon p(S^*) = (1 - \epsilon)p(S^*)\]
              <br>
              Therefore we know the this algorithm must give us an optimal solution at least \(\epsilon\) portion of the optimality. 
              <br>
              Furthermore, it also gives us an upperbound on the solution which is: 
              \[
                \frac{p'(\tilde{S})}{1 - \epsilon} \geq p(S^*)
              \]
              and then: 
              \[
                \frac{p'(\tilde{S})}{m(1 - \epsilon)} \geq \frac{p(S^*)}{m} = Opt_{abs}
              \]
              Which is an upper bound, as \(\epsilon \rightarrow 0\), we know that \(m\) increases together with 
              \(p'(\tilde{S})\), and it will eventually be equal because the round error gets less as the multiplier increase. 
            </p>
            <h3>Claim II:</h3>
            <p>
              There is another rounding strategy that might appear to be more appealing but I don't know yet how to prove it. 
              The new multiplier is set to separate profits in such a way that after scaling, item's profits lies within 
              their integer slots. Then m can be defined as: 
              \[
                m :=
                \frac{2}
                {
                  \underset{0 \leq i, j \leq n - 1}{\text{min}}
                  \{|p_i - p_j|\}
                }
              \]
            </p>
            <p>
              We need to assume that the profits of each items are unique to avoid denominator being zero. 
              In the case where all tiems has the same profits, no scaling and rounding is needed.
            </p>
            <p>
              This scaling strategy will asserts that all the item's profits preserves it's orderings after the scaling, 
              this will produce a smaller scaling factor and it "seems to" preserve the optimal solution
              (please contact me if you know how to prove or disprove this). 
            </p>
          </p>
        </p>
        <h2>Primal with Rounding</h2>
        <p>
          Primal Rounding rounds the weights of the items to integer down, and then use DP to solve. This is generally
          a good idea however changing the constraints of the function can devistating and it might even break
          one of the preconditions listed. 
        </p>
        <p>
          The round strategy seek for a way of putting all the weights of the items into their unique integer 
          slot so that the oderings of the item's wieghts are preserved the scaling and rounding. In this case, assuming
          weights of all items are unique: 
          \[
            m :=
            \frac{2}
            {
              \underset{0 \leq i, j \leq n - 1}{\text{min}}
              \{|w_i - w_j|\}
            }
          \]
        </p>
        <h3>Claim I:</h3>
        <p>
          Round the weights of the items down can give an upperbound as that is pretty off. 
        </p>
        <h4>An Instance: </h4>
        <p>
          Consider these 2 items with the following weights: 
          \[(1.099 + 0.1, 1.099 - 0.1)\]
          Then it's not hard to determine that the scaling factor m will be: \(m=10\)
          Then after scaling, we have the following weights for the items: 
          \[(10.99 + 1, 10.99 - 1)\]
          Rounding things down, we have: 
          \[(11, 9)\] 
          where each item's weight is off by 0.99. Now let's recover the weight by dividing the rounded value by the 
          multipler to get: 
          \[(1.1, 0.9)\]
          and then each weight of the item will be off by +0.099. 
        </p>
        <p>
          Then in this case, the weight can be off by 10% when solving it based on the rounded weight. 
          <br>
          The key here is to choose items only taking 2 value, and choosing the value in such a way that it maximizes the 
          rounding error. 
        </p>
        <p>
          Now consider a more extreme case where the weights of the item only takes on 2 values and they are: 
          \(\{1.499 + 0.5, 1.499 - 0.5\}\), then in this case the scaling factor will be: m = 1, hence we can round thing 
          down directly to get: \(\{1, 0\}\). Then in this case, the weight of each item is off by 49.9%. 
          <br>
          Now imagine we have a list of items taking the values of: 
          \[\lbrack 1.999, 0.999, 1.999, 0.999... \rbrack\]
          Repeating 2n times, then the errors will be able to build up. Suppose the budget \(b\) we have is 2n, also assume that 
          items with weight \(0.999\) has no profits, then after rounding it, the budget will remain unchanged as \(n\) gets 
          arbitrarily large while the weight is reduced by 50%, allowing us to include 2 times many items as the actual optimal solution.  
          <br>
          Therefore, in the worst case, the upperbound will be 1/2 of the optimal value of the solution. Comparatively 
          this is way slower than the greedy algorithm and yet they have equally bad worst case. However how likely 
          for each of them to hit the worst case worth another lengthy discussion; but I believe it's unlikely. 
        </p>
        <h4>Claim II:</h4>
        <p>(There is a way to place a better bound on the output, but it will depend on the inputs of the algorithm. )</p>
        <h2>Why All These?</h2>
        <p>
          We are particularly interested in the approximation algorithm because a tight upperbound will 
          serve the Branch and Bound algorithm well. 
        </p>
        <p>
          It's not smart to use the same approximation algorithm for branch a bound either, depending on the particular inputs,
          some heuristic works better than others. Hence, if we can mix all the approximation algorithm together to produce a best 
          upperbound for the algorithm, then it will speed up the run-time of the Branch and Bound algorithm. 
        </p>
      </section>
      <hr>
      <section id="bb">
        <h2>Branch & Bound Algorithm</h2>
          <p>
            The branch and bound algorithm uses a heuristic algorithm that produces an upper bound 
            for maximization problem. Here we are going to use different heuristics to aid 
            with the algorithm.
          </p>
          <h3>
            Determing Factors for B&B: 
          </h3>
          <p>
            A branch and bound algorithm is an improvement on bruteforce search algorithm 
            for NP-Hard problem, instead of searching all the solution spaces, it uses heuristic 
            algorithms that is fast(Approximation algorithms) to determine which subset 
            of solutions spaces it's going to investigate.
            <br>
            2 of the most important ingredient of the branch and bound algorithm are: 
            <ul>
              <li>The way order of branching.</li>
              <li>The topology of growing the tree.</li>
              <li>The precision of the Heuristic</li>
            </ul>
            <br>
            The easier heuristic to use is the greedy algorithm, it's fast, but it's not always 
            very accurate.
        </p>
        <h3>Demo Psuedocodes: </h3>
        <p>
          Define a class name: Problem 
          <p>
            Which has the following variables in its field: 
          </p>
            <ul>
              <li>
                I1: List of Items's Index, representing the list of items that are added to the knapsack by the supper-problem 
                (Or, the Caller of a recursive function)
              </li>
              <li>
                I2: List of Item's Index, representing the list of items that are free to invesitage by this sub-problem. 
              </li>
              <li>
                P: List of profits
              </li>
              <li>
                W: List of weights
              </li>
              <li>
                Budget
              </li>
            </ul>
          <p>
            And it has one method that is defined as the following: 
          </p>
          <ul>
            <li>
              greedy(): Choose, among all items marked by I2, and returns the following
              <ul>
                <li>
                  IntIdx: The list of indices of items that are choosen as a whole by the greedy algorithm.
                </li>
                <li>
                  Frac: The index of the item choosen to have non-integer solution to it. 
                </li>
                <li>
                  Opt: The upper bound on the given problem (doesn't have to be given by Greedy)
                </li>
              </ul>
            </li>
          </ul>
        </p>
        <h4>Psuedo Codes: </h4>
        <p>
          Inputs: 
          <ul>
            <li>
              P: An instance of a problem. 
            </li>
            <li>
              S_star: I1 of the given problem P, list of indices that are aready in the knapsack by the problem. 
            </li>
            <li>
              Opt_star: total profits of all the items that are included into the Knapsack. 
            </li>
          </ul>
        </p>
        function: 
        <div class="indent-all">
          def soln(P: Type[Problem], S_star, Opt_star): 
            <div class="indent-all">
              stack = {P} <br>
              while stack is not empty: 
                <div class="indent-all">
                  P0 = stack.pop() <br>
                  IntIdx, Frac, Opt = P0.greedy() <br>
                  if Opt > Opt_star: 
                  <div class="indent-all">
                    if frac is None: 
                    <div class="indent-all">
                      Opt_star := Opt // solution is all integers
                    </div>
                    else: 
                    <div class="indent-all">
                      stack.push(Problem(I + [frac], I2.remove(frac), P0.P, P0.W, P0.B - P0.W[frac])) <br>
                      stack.push(Problem(I, I2.remove(frac), P0.P, P0.W, P0.B - P0.W[frac]))
                    </div>
                  </div>
                </div>
                <div class="indent-all">
                  if IntIdx has better profits than S_star: <br>
                  S_star = P0.I1 + IntIdx
                </div>
            </div>
            return S_star, Opt_star
        </div>
        <h3>Additional Notes</h3>
        <p>
          When the heuristics are set to be the greedy algorithm, this branch and bound algorithm become equivalent to 
          the solving the same system of inqaulities using the Branch and Bound algorithm for Linear inqaulities. 
        </p>
        <p>
          A mixed heuristics can be used, however we need to identify the characteristics from a sub-proboem such that, 
          using approximation algorithms will give a better run-time than the The greedy algorithm. 
        </p>
        <p>
          Finally, a "warm-start" feasible solution can be provided in advance to speed up the branch and bound process,
          this is helpful because a high value for the global objective value will cut some of the branching process. 
        </p>
        <p>
          However in my code, I used the integral part of the greedy solution to give a warm start to the 
          Branch and Bound algorithm, which is the most efficient to use. 
        </p>
        <h3>
          Pathological Inputs
        </h3>
        <p>
          We claim that the BB algorithm is exponential, however it's relatively rare occurences for realy life problem. 
          This particular Pathological instance works the same for BB algorithm for linear programming and it's taken 
          from the course notes of math 409, credits goes to Thomas Rothvoss, a professor at UW.
        </p>
        <p>
          The pothological instance is formulated as the following: 
          \[
            \max \lbrace 
              x_0|
                \frac{x_0}{2} + \sum_{i = 1}^n = \frac{n}{2}; x \in \{0, 1\}^{n+1} 
            \rbrace
          \]
          With the constraint that \(n \geq 4\) and n is even (odd number of items because index starts with zero). 
        </p>
        <h3>proof: </h3>
        <p>
          Here, we assume that, a warm start solution for the problem is not given on the start of the algorithm. 
        </p>
        <p>
          Each node on the BB tree represents a sub-problem where some of the value for \(x_i\) is fixed, and hence. Model 
          it with 2 index sets: \(I_0\) and \(I_1\). 
        </p>
        <p>
          For at each node, we have a candidate solution: \(\tilde{x}_i\) and it's defined as the following: 
          \[
            \tilde{x}_i := 
            \begin{cases}
              1 & i = 0\\
              1 & i \in I_1\\
              0 & i \in I_0\\
              \lambda & else
            \end{cases}
          \]
        </p>
        <p>As long as \(|I_0| + |I_1| \leq \frac{n}{3}\), and \(\tilde{x}_0 = 1\), then there is a feasible LP that cannot be pruned yet. </p>
        <p>
          Exam the following expression: 
          \[
            \frac{\tilde{x}_0}{2} + \sum_{i=1}^n \tilde{x}_i
            = 
            \frac{1}{2} + |I_1| + (n - |I_0| - |I_1|)\lambda = \frac{n}{2}
          \]
          Where the quantity: \(n - |I_0| - |I_1|\) the variable that are still free and can be fractional. 
        </p>
        <p>
          Which gives: 
          \[
            \sum_{i=1}^n \tilde{x}_i
            = 
            |I_1| + (n - |I_0| - |I_1|)\lambda = \frac{n}{2} - \frac{1}{2}
          \]
        </p>
        <p>
          Remember that: \(0 \leq |I_1| \leq \frac{n}{3}\) and \(|I_0| + |I_1| \leq \frac{n}{3}\) which implies that: 
          \[
            n - |I_0| - |I_1| \geq \frac{2n}{3}
          \]
        </p>
        <p>
          There always exists a choice of \(\lambda\) in between zero an one such that the sub-problem is solvable. In addtion, 
          the solution to the sub-problem will be a fractional value too because of the assumption that n is even and the 
          \(1/2\) on the right hand side, therefore, the solution is also non-integer. 
        </p>
        <p>
          As a result, the tree will keep branching for \(n/3\) in depth, meaning that the algorithm will be exponential in runtime. 
        </p>
      </section>
      <hr>
      <section id="lp">
        <h2>Knapsack but Linear Programming</h2>
        <p>
          This is a linear programming problem in its nature, and all above discussion is only for mathematical interests
          however in practices, the reduction of this problem to linear programming is the easiest once a solver avaible. 
        </p>
        <h3>Extended Knapsack</h3>
        <p>
          This problem so far deal with choosing an item that is 0/1, means for each item, we only have the choice 
          to: include, or not include. 
          However, as we already show that, inside the approximation algorithm, items can be rounded to the same weights, 
          and it it's equivalent of having the same items, but with non-binary variable. 

          Hence the new Knapsack probem is concerned with using an item multiple times, or more specifically: 
          $$x_i \in \lbrack 0, c_i\rbrack \cap \mathbb{Z}_+$$
          Which is the only changes made to the 0/1 Knapsack Problem. 
        </p>
        <p>
          So, I think I found a specialized way of adapting the LP Simplex Algorithm with the BB that might speed up
          the running time for the branch and bound, against the Naive implementation of the algorithm, but at the time
          of writing the website, this algorithm is still in development.
        </p>
        <h3>Branch and Bound</h3>
        <p>
          It's up to the reader to realize the trivial fact that the branch and bound algorithm for linear programming is equivalent 
          to the branch and bound algorithm using greedy approximation as the heuristic. 
        </p>
        <h3>Gomory Cut</h3>
        <p>
          It's left as an excercise for the readers to comtemplate the intuitive connection between using scaling and rounding
          as heuristic and Branch and Bound algorithm versus using Gomory cut on the linear programming problem. 
        </p>
      </section>
      <hr>
      <section id="bench">
        <h2>Performance & Banchmark</h2>
        <p>
          For some readers this along with the intro might be the the only parts that they are interested in reading about. 
        </p>
        <p>
          <div id="ploty-module"> 
          </div>
        </p>
        <p>
          Exactly, This is some huge disparity in time comsuption, so here I will link all the python codes that is 
          involved in  making up this plot and then explain how it works. 
        </p>
        <pre><code class="python">
from random import random as rnd
from numpy import random as np_rnd
from time import time
from knapsack.core import *
import sys; import os
from quick_csv import core
from quick_json import quick_json


def rand_problem_ints(range_:int, itemsCount:int, sparseness):
    assert 0 < sparseness < 1
    weights = [int(rnd() * range_) for I in range(itemsCount)]
    profits = [int(rnd() * range_) for I in range(itemsCount)]
    MaxWeights = int(sparseness * (sum(weights)))
    return profits, weights, MaxWeights


def run_solve_on(problemList, solver:callable):
    Time, Opt = [], []
    for P, W, B in problemList:
        Start = time()
        Value = solver(P,W,B)
        Time.append(time() - Start)
        print(f"Timer: {Time[-1]}")
        Opt.append(Value)
    return Time, Opt


def rand_problem_exponential(scale: int, itemsCount: int, satruration):
    assert 0 < satruration < 1
    Profits, Weights = np_rnd.exponential(scale=scale, size=(2, itemsCount))
    Profits, Weights = map(lambda x: int(x), Profits), map(lambda x:int(x), Weights)
    Profits, Weights = list(Profits), list(Weights)
    Budget = int(satruration*sum(W for W in Weights))
    return Profits, Weights, Budget


def bench_bb_with_dp(trials:int):

    def dp_solve(P, W, B):
        _, Opt = knapsack_dp_dual(P,W,B)
        return Opt

    def bb_solve(P, W, B):
        _, Opt = branch_and_bound(P, W, B)
        return Opt

    ItemCount = 20
    ItemProfitsWeightsRange = 1000
    KnapSackSparseness = 0.1
    ProblemList = [rand_problem_ints(ItemProfitsWeightsRange, ItemCount, KnapSackSparseness) for P in range(trials)]
    ProblemList += [rand_problem_exponential(ItemProfitsWeightsRange, ItemCount, KnapSackSparseness) for P in range(trials)]
    bb_time, bb_opt = run_solve_on(ProblemList, bb_solve)
    dp_time, dp_opt = run_solve_on(ProblemList, dp_solve)
    CSVHeader = ["bb_time", "dp_time", "bb_opt", "dp_opt"]
    CSVCols = [bb_time, dp_time, bb_opt, dp_opt]
    def MakeJsonResults():
        Json = {}
        Json["bb_time"], Json["dp_time"] = bb_time, dp_time
        return Json
    core.csv_col_save("bb, dp bench.csv", colHeader= CSVHeader, cols=CSVCols)
    quick_json.json_encode(MakeJsonResults(), filename="bb, dp bench.json")


    print("Tests Detailed: ")
    print(f"The same set of problem is run on both BB and DP, time and optimal value is recored. ")
    print("The optimal value for both solver should be the same and the time cost is the interests. ")
    print(f"Item Count: {ItemProfitsWeightsRange}, Item's Profits and Weight range: (0, {ItemProfitsWeightsRange}), "
          f"Knapsack Sparseness: {KnapSackSparseness}")


def main():
    bench_bb_with_dp(10)
    pass


def test():
    print(rand_problem_exponential(10000, 5, 0.5))


if __name__ == "__main__":
    print(f"swd: {os.getcwd()}")
    print(sys.argv)
    if len(sys.argv) != 1:
        test()
    else:
        main()

        </code></pre>
        <p>
          To compare the performance of both the BB and the DP algorithm, items with integers weights and profits are used, 
          but they are in very huge value so that it's comparable to items with profits and weights as float values. 
        </p>
        <p>
          There are 2 major ways a random problem are generated, by the "rand_problem_ints" and the "rand_problem_exponential" 
          method in the script. 
          <ul>
            <li>
              rand_problem_exponential
              <ul>
                <li>This method generates item's with weights and profits from a uniform distribution. </li>
              </ul>
            </li>
            <li>rand_problem_exponential</li>
            <ul>
              <li>This method generates item's weights and profits from a exponential distribution. </li>
            </ul>
          </ul>
        </p>
      </section>
      <section id="epilogue">
        <h2>
          Epilogue
        </h2>
        <p>
           We have explored 2 of the ways of how dynamic programming can solve the problem, how rounding the floats values
          allowed us to approximate the solution for the problem and use it to as heuristic for the Branch and Bound algorithm 
          in the end. 
        </p>
        <p>
          Throughout the exploration, we observe the intricacy of the zero one knapsack problem and revealed the fact that, 
          Dynamic Programming is just the beginning of things and how this problem in particular, can be a very good 
          reduced example for LP, where, the simplex solving routine has been substituted with the Heuristic we developed in the session. 
        </p>
        <p>
          I wish to write more on this, but have to pause the project here because of time limitations. There are a still a 
          lot of problems/subjects left unexplored such as:
          <ul>
            <li>Mixed Heuristic for BB</li>
            <li>Extended knapsack</li>
            <li>More Benchmarking</li>
            <li>Upper Bound for Primal Rounding</li>
            <li>Unproved Precise Scaling Strategy in Rounding Primal</li>
          </ul>
          And much much more connections to discover from this problem to other area of mathematics and computer algorithms. 
        </p>
      </section>
      <div class="footer-padding">
      </div>
    </div>
    <footer>
      <!--For MathJax-->
      <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      <script id="MathJax-script"
            src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js">
      </script>
      <!--For MathJax Ends-->
    </footer>
  </body>
</html>