<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">
    <!--Edit Here-->
    <title>WRITE ME</title>
    <!--Open graph protocol EDIT HERE-->
    <meta property="og:title" content="WRITE ME">
    <meta property="og:type" content="article">
    <meta property="og:description"
    content="WRITE ME">
    <meta property="og:image" content="https://iluvjava.github.io/krylov_subspace_cg/energy_norm.png" >
    <!--END-->
    <!--Edit Here -->
    <!-- ForTwitter-->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="WRITE ME" />
    <meta name="twitter:description" content="WRITE ME">
    <!---->
    <!--For Bootstrap style-->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
        integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!--Bootstrap end-->

    <!--MyStyle-->
    <link rel="stylesheet" href="mystyle.css">
    <!--End-->

    <!--For plotting data and Adpating Webpage to mobile devices-->
    <script src="client_scripts_general.js"></script>
    <!-- End -->

    <!--Plotly-->
    <script src="https://cdn.plot.ly/plotly-1.2.0.min.js"></script>
    <!--FIX VERSION, new UPDATE has problem-->
    <!--END-->

    <!--JS hilight for the code blocks in the web: -->
    <link rel="stylesheet" href="../assets/client_scripts/syntax_highlight_pack/styles/a11y-light.css">
    <script src="../assets/client_scripts/syntax_highlight_pack/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <!--END-->
    <!--For Katex -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js" integrity="sha384-YNHdsYkH6gMx9y3mRkmcJ2mFUjTd0qNQQvY9VYZgQd7DcN7env35GzlmFaZ23JGp" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
            // customised options
            // • auto-render specific keys, e.g.:
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false},
                {left: '\\(', right: '\\)', display: false},
                {left: '\\[', right: '\\]', display: true}
            ],
            // • rendering keys, e.g.:
            throwOnError : false
            });
        });
    </script>
    <!--END-->
</head>
<body>
    <nav class="navbar sticky-top">
        <div class="container-fluid">
            <div class="navbar-header">
                <!-- Edit here -->
                <a class="navbar-brand" href="../index.html">My WebPages</a>
            </div>
            <botton>
                <span class="nav-hamburger-menu-icon hide"><img src="../assets/menu_hamburger.png" alt="menu"></span>
            </botton>
            <div class="collapse navbar-collapse">
                <ul>
                    <!--Section links. Edit Here -->
                </ul>
            </div>
        </div>
    </nav>
    <div id="side-bar">
        <ul>
            <!--Section links. Edit Here-->
        </ul>
    </div>
    <div id="main-content">
        <section id="intro">
            <h2>Intro</h2>
            <p>
                Here is a list of content that we will go through in this website. 
                <ul>
                    <li>The Lasso Problem</li>
                    <li>Quadratic Programming</li>
                    <li>Proximal Gradient Descend</li>
                    <li>Julia Programming Language for Speed: Home Made Accelerated Proximal Gradient</li>
                    <li>Results</li>
                    <li>Epilogue</li>
                </ul>
            </p>
            <p>
                Major Idea and Lessons
                <blockquote>
                    Don't use Jump.jl framework it's slow. The reason is due to repeated computations of $A^TA$ of the data matrix, and reallocated memory each time it does it, which is just my hypothesis based on the memory usage. Although proximal gradient is fast and can handle all type of reguarized solution, but there are some details missing for a accurate predictions on the importance of predictors. 
                </blockquote>
            </p>
        </section>
        <hr>
        <section>
            <h2>The Lasso Problem</h2>
            <p>
                Here we use a basic regression problem without biases, then the lasso is just a regularizing the one-norm of the weights of the model. Mathematically we are solving: 
            </p>
            <p>
                <blockquote>
                $$
                \arg\min_x \left\lbrace
                    \Vert Aw - b\Vert_2^2 + \lambda \Vert w\Vert_1
                \right\rbrace
                $$
                </blockquote>
            </p>
            <p>
                Assume that the size of the data matrix $A$ has zero mean on all the columns, and it's a $n\times d$ matrix, having $n$ rows of instances, where each instances has $d$ features. 
            </p>
            <h3>Intutions</h3>
            <p>
                By solving this optimization problem with different value of $\lambda$, we are subjecting the model to produce weights for predictors that are mostly zero. Such a strategy will make sparse models, and only the important predictors will remain. 
            </p>
            <p>
                As the size of $\lambda$ increases, the model is forced to set the weights for these 2 type of predictors to zero: 
                <ul>
                    <li>Predictors that has collinearity with other predictors.</li>
                    <li>Predictors that naturally has small weigths to it.</li>
                </ul>
                Probabalistically, the regurlarized problem is changing the prior distribution of the weights into the Laplace Distribution. 
            </p>
            <h3>The Lasso Path</h3>
            <p>
                For the 2norm loss function, the lambda value such that it sets all the weights of the predictors to zero is given as: 
            </p>
            <p>
                <blockquote>
                    $$
                    \lambda_{\max} = 
                    \max_{k=1,...,d} \left\lbrace
                        2 \left|
                            \sum_{i = 1}^{n} A_{i, k}
                            \left(
                                b_i - \left(
                                    \frac{1}{n} \sum_{j=1}^{i} b_i
                                \right)
                            \right)
                        \right|
                    \right\rbrace
                    $$
                </blockquote>
            </p>
            <p>
                The proof is skipped and I will give a hint about it later. 
            </p>
            <p>
                By choosing $\lambda_{\max}$ to be this value, and then we decrease the value of $\lambda$ geometrically, producing a sequence of weights for each value of $\lambda$, and then each of the weights on a line, for all of the weights, this is called the <b>Lasso Path</b>. 
            </p>
        </section>
        <hr>
        <section id="qp">
            <h2>Quadratic Programming</h2>
            <h3>Direct Formulation</h3>
            <p>
                The lasso problem from above can be simplified into a quadratic programming problem as the following: 
                
            </p>
            <p>
                <blockquote>
                    $$
                    \arg\min_w \left\lbrace
                        \lambda \mathbf{1}^T\eta + \Vert Aw - b\Vert_2^2 : 
                        -\eta \le x \le \eta
                    \right\rbrace
                    $$
                </blockquote>
            </p>
            <p>
                Where the variable $\eta$ is a vector that has the same size as the vector $w$. This is a quadratic programming problem. 
            </p>
            <h3>The Dual Problem</h3>
            <p>Another way is to use the Dual problem of the lasso, we will skip the dualization process and just state the results here: </p>
            <p>
                <blockquote>
                    $$
                    \sup_{\Vert A^Tz\Vert_\infty \le \lambda} 
                    \left\lbrace
                        -z^Tb - \frac{1}{2}\Vert z\Vert^2
                    \right\rbrace
                    $$
                </blockquote>
            </p>
            <p>
                And we would also need the Lagrangian for it to determine the solutions for the primal. 
            </p>
            <p>
                Under the context of the quadratic programming problem, we will need to change the $\lambda$ repreatedly, hence a warm start using the preivous primal solution calculated with the nearest value of $\lambda$ will speed up the algorithm. 
            </p>
            <h3>Code, Tools, Implementations and Practical Barriers</h3>
            <p>
                A quadratic programming problem can be solved efficiently with the Interior Points method, let's see if I have the time to implement this. For this project, I use the <code>JuMP.jl</code> as the optimization interface and <code>COSMO.jl</code> as the underlying solver, which is an ADMM based Conic Programming solver from Standford University. It's applicable for quadratic objective and convex quadrtic constraints, and a lot of other constraints such as Semi-Definitness of the variable matrix. 
            </p>
            <p>
                <h4>A Statement of the Challenges and Problems</h4>
                <blockquote>
                    The speed of solving from using the <code>JuMP.jl</code> with <code>COSMO.jl</code> was disappointing, due to too many memory allocations, terrible speed at loading up the packages, and assigning variable. In addition, it has numerical instability that is visiable on the plot of the lasso path plot, manifested as tiny artifects on the plos.
                </blockquote>
                
            </p>
            <p>
                Here is what I should do in the future if I were to address these problem: 
                <ul>
                    <li>Directly Communicate with the solver such as <code>COSMO.jl</code> instead of using any kind of stupid interface such as <code>JuMP.jl</code>, if, heavy modifications of the optimization problems are involved</li>
                    <li>
                        Make my own interior points solver, which is not hard. However, I am also giving up development time, and other valuable features and generality provided by the conic Solver, such as the <code>COSMO.jl</code>. 
                    </li>
                </ul>
            </p>
            <h3>By Passing the Problem all Together</h3>
            <p>
                This leads to the next part of the article. There is another state of the art solution that is easier to implement, and when implemented in julia, it <b> produces up to 500 times speed up compare to the <code>JuMP.jl + COSMO.jl</code> approach </b>. It's called the Proximal Gradient Descend Method.
            </p>
        </section>
        <div class="footer-padding">
        </div>
    </div>
    <footer>
        <!--For footer context botton-->
        <div class="fixed-bottom hide" id="footer-display">
            <div class="row align-items-center" id="footer-display-inner">
                <div class="col text-right">
                    <button type="button" class="btn btn-primary toggle-btn">Toggle Big Printout</button>
                </div>
            </div>
        </div>
        <!--Ends-->
    </footer>
</body>
</html>