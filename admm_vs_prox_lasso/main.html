<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">
    <!--Edit Here-->
    <title>Lasso Path with Proximal Gradient</title>
    <!--Open graph protocol EDIT HERE-->
    <meta property="og:title" content="Lasso Path with Proximal Gradient">
    <meta property="og:type" content="article">
    <meta property="og:description"
    content="The frustrations with ADMM based Conic Solver and Julia Optimization Frameworks sparks my own efficient implemention of accelerated proximal gradient descend for the lasso path problem.">
    <meta property="og:image" content="https://iluvjava.github.io/admm_vs_prox_lasso/lasso_proximal.png" >
    <!--END-->
    <!--Edit Here -->
    <!-- ForTwitter-->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Lasso Path with Proximal Gradient"/>
    <meta name="twitter:description" content="The frustrations with ADMM based Conic Solver and Julia Optimization Frameworks sparks my own efficient implemention of accelerated proximal gradient descend for the lasso path problem.">
    <!---->
    <!--For Bootstrap style-->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
        integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!--Bootstrap end-->

    <!--MyStyle-->
    <link rel="stylesheet" href="mystyle.css">
    <!--End-->

    <!--For plotting data and Adpating Webpage to mobile devices-->
    <script src="client_scripts_general.js"></script>
    <!-- End -->

    <!--Plotly-->
    <script src="https://cdn.plot.ly/plotly-1.2.0.min.js"></script>
    <!--FIX VERSION, new UPDATE has problem-->
    <!--END-->

    <!--JS hilight for the code blocks in the web: -->
    <link rel="stylesheet" href="../assets/client_scripts/syntax_highlight_pack/styles/a11y-light.css">
    <script src="../assets/client_scripts/syntax_highlight_pack/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <!--END-->
    <!--For Katex -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js" integrity="sha384-YNHdsYkH6gMx9y3mRkmcJ2mFUjTd0qNQQvY9VYZgQd7DcN7env35GzlmFaZ23JGp" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
            // customised options
            // • auto-render specific keys, e.g.:
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false},
                {left: '\\(', right: '\\)', display: false},
                {left: '\\[', right: '\\]', display: true}
            ],
            // • rendering keys, e.g.:
            throwOnError : false
            });
        });
    </script>
    <!--END-->
</head>
<body>
    <nav class="navbar sticky-top">
        <div class="container-fluid">
            <div class="navbar-header">
                <!-- Edit here -->
                <a class="navbar-brand" href="../index.html">My WebPages</a>
            </div>
            <botton>
                <span class="nav-hamburger-menu-icon hide"><img src="../assets/menu_hamburger.png" alt="menu"></span>
            </botton>
            <div class="collapse navbar-collapse">
                <ul>
                    <!--Section links. Edit Here -->
                    <li><a href="#intro">Intro</a></li>
                    <li>
                        <a href="#the-lasso-problem">The Lasso Problem</a>
                        <ul>
                            <a href="#the-lasso-path">The Lasso path</a>
                        </ul>
                    </li>
                    <li><a href="#qp">Quadratic Programming</a></li>
                    <li>
                        <a href="#pgd">Proximal Gradient Descend</a>
                        <ul>
                            <li><a href="#the-proximal-gradient">The Proximal Gradient</a></li>
                            <li>
                                <a href="#claim-1">Claim 1</a>
                            </li>
                            <li>
                                <a href="#formulation-of-the-algorithm">Formulation of the Algorithm</a>
                            </li>
                            <li>
                                <a href="#proximal-operator-for-the-l1-regularization">Proximal Opt for L1-Regularization</a>
                            </li>
                        </ul>
                    </li>
                    <li>
                        <a href="#julia">The Julia Programming Language and my Code</a>
                        
                    </li>
                    <li><a href="#res">Results</a></li>
                    <li>
                        <a href="#epilogue">Epilogue</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>
    <div id="side-bar">
        <ul>
            <!--Section links. Edit Here-->
            <li><a href="#intro">Intro</a></li>
            <li>
                <a href="#the-lasso-problem">The Lasso Problem</a>
                <ul>
                    <a href="#the-lasso-path">The Lasso path</a>
                </ul>
            </li>
            <li><a href="#qp">Quadratic Programming</a></li>
            <li>
                <a href="#pgd">Proximal Gradient Descend</a>
                <ul>
                    <li><a href="#the-proximal-gradient">The Proximal Gradient</a></li>
                    <li>
                        <a href="#claim-1">Claim 1</a>
                    </li>
                    <li>
                        <a href="#formulation-of-the-algorithm">Formulation of the Algorithma</a>
                    </li>
                    <li>
                        <a href="#proximal-operator-for-the-l1-regularization">Proximal Opt for L1-Regularization</a>
                    </li>               
                </ul>
            </li>
            <li>
                <a href="#julia">The Julia Programming Language my and 
                    Code</a>
            </li>
            <li><a href="#res">Results</a></li>
            <li>
                <a href="#epilogue">Epilogue</a>
            </li>
        </ul>
    </div>
    <div id="main-content">
        <section id="intro">
            <h2>Intro</h2>
            <p>
                Here is a list of content that we will go through in this website. 
                <ul>
                    <li>The Lasso Problem</li>
                    <li>Quadratic Programming</li>
                    <li>Proximal Gradient Descend</li>
                    <li>Julia Programming Language for Speed: Home Made Accelerated Proximal Gradient</li>
                    <li>Results</li>
                    <li>Epilogue</li>
                </ul>
            </p>
            <h3>A Summary: What, Why, How</h3>
            <p>
                I want to do the lasso path problem by formulating it as a quadratic programming problem, and then try solving it using the <code>JuMP.jl</code> optimization framework with the <code>COSMO.j</code> ADMM conic solver. However it turns out to be very slow, even with everything precompiled. I tried the following things and it didn't rescue it: 
                <ul>
                    <li>Using primal variable warm start for the solver</li>
                    <li>Less Tolerance</li>
                    <li>Build the model in <code>JuMP.jl</code> once and then modify the objective, reduce resource overhead on creating new model for each $\lambda$</li>
                </ul>
                <br>
                Then I suddenly remebered that, Proximal gradient can be directly applied to the Primal, Dual of the non-smooth objective function, and achieve convergence as good as $\mathcal{O}(\exp(-k))$. 
                <br>
                With some good programming skill, I am able to get it working very efficiently, more precisely, 500 times speed up. 
            </p>
            <p>
                <h3>Major Idea and Lessons</h3>
                <blockquote>
                    Don't use Jump.jl framework it's slow. The reason is due to repeated computations of $A^TA$ of the data matrix, and reallocated memory each time it does it, which is just my hypothesis based on the memory usage. Although proximal gradient is fast and can handle all type of reguarized solution, but there are some details missing for a accurate predictions on the importance of predictors. 
                </blockquote>
            </p>
        </section>
        <hr>
        <section id="the-lasso-problem">
            <h2>The Lasso Problem</h2>
            <p>
                Here we use a basic regression problem without biases, then the lasso is just a regularizing the one-norm of the weights of the model. Mathematically we are solving: 
            </p>
            <p>
                <blockquote>
                $$
                \arg\min_x \left\lbrace
                    \Vert Aw - b\Vert_2^2 + \lambda \Vert w\Vert_1
                \right\rbrace
                $$
                </blockquote>
            </p>
            <p>
                Assume that the size of the data matrix $A$ has zero mean on all the columns, and it's a $n\times d$ matrix, having $n$ rows of instances, where each instances has $d$ features. 
            </p>
            <h3>Intutions</h3>
            <p>
                By solving this optimization problem with different value of $\lambda$, we are subjecting the model to produce weights for predictors that are mostly zero. Such a strategy will make sparse models, and only the important predictors will remain. 
            </p>
            <p>
                As the size of $\lambda$ increases, the model is forced to set the weights for these 2 type of predictors to zero: 
                <ul>
                    <li>Predictors that has collinearity with other predictors.</li>
                    <li>Predictors that naturally has small weigths to it.</li>
                </ul>
                Probabalistically, the regurlarized problem is changing the prior distribution of the weights into the Laplace Distribution. 
            </p>
            <h3 id="the-lasso-path">The Lasso Path</h3>
            <p>
                For the 2norm loss function, the lambda value such that it sets all the weights of the predictors to zero is given as: 
            </p>
            <p>
                <blockquote>
                    $$
                    \lambda_{\max} = 
                    \max_{k=1,...,d} \left\lbrace
                        2 \left|
                            \sum_{i = 1}^{n} A_{i, k}
                            \left(
                                b_i - \left(
                                    \frac{1}{n} \sum_{j=1}^{i} b_i
                                \right)
                            \right)
                        \right|
                    \right\rbrace
                    $$
                </blockquote>
            </p>
            <p>
                The proof is skipped and I will give a hint about it later. 
            </p>
            <p>
                By choosing $\lambda_{\max}$ to be this value, and then we decrease the value of $\lambda$ geometrically, producing a sequence of weights for each value of $\lambda$, and then each of the weights on a line, for all of the weights, this is called the <b>Lasso Path</b>. 
            </p>
        </section>
        <hr>
        <section id="qp">
            <h2>Quadratic Programming</h2>
            <h3>Direct Formulation</h3>
            <p>
                The lasso problem from above can be simplified into a quadratic programming problem as the following: 
                
            </p>
            <p>
                <blockquote>
                    $$
                    \arg\min_w \left\lbrace
                        \lambda \mathbf{1}^T\eta + \Vert Aw - b\Vert_2^2 : 
                        -\eta \le x \le \eta
                    \right\rbrace
                    $$
                </blockquote>
            </p>
            <p>
                Where the variable $\eta$ is a vector that has the same size as the vector $w$. This is a quadratic programming problem. 
            </p>
            <h3>The Dual Problem</h3>
            <p>Another way is to use the Dual problem of the lasso, we will skip the dualization process and just state the results here: </p>
            <p>
                <blockquote>
                    $$
                    \sup_{\Vert A^Tz\Vert_\infty \le \lambda} 
                    \left\lbrace
                        -z^Tb - \frac{1}{2}\Vert z\Vert^2
                    \right\rbrace
                    $$
                </blockquote>
            </p>
            <p>
                And we would also need the Lagrangian for it to determine the solutions for the primal. 
            </p>
            <p>
                Under the context of the quadratic programming problem, we will need to change the $\lambda$ repreatedly, hence a warm start using the preivous primal solution calculated with the nearest value of $\lambda$ will speed up the algorithm. 
            </p>
            <h3>Code, Tools, Implementations and Practical Barriers</h3>
            <p>
                A quadratic programming problem can be solved efficiently with the Interior Points method, let's see if I have the time to implement this. For this project, I use the <code>JuMP.jl</code> as the optimization interface and <code>COSMO.jl</code> as the underlying solver, which is an ADMM based Conic Programming solver from Standford University. It's applicable for quadratic objective and convex quadrtic constraints, and a lot of other constraints such as Semi-Definitness of the variable matrix. 
            </p>
            <p>
                <h4>A Statement of the Challenges and Problems</h4>
                <blockquote>
                    The speed of solving from using the <code>JuMP.jl</code> with <code>COSMO.jl</code> was disappointing, due to too many memory allocations, terrible speed at loading up the packages, and assigning variable. In addition, it has numerical instability that is visiable on the plot of the lasso path plot, manifested as tiny artifects on the plos.
                </blockquote>
                
            </p>
            <p>
                Here is what I should do in the future if I were to address these problem: 
                <ul>
                    <li>Directly Communicate with the solver such as <code>COSMO.jl</code> instead of using any kind of stupid interface such as <code>JuMP.jl</code>, if, heavy modifications of the optimization problems are involved</li>
                    <li>
                        Make my own interior points solver, which is not hard. However, I am also giving up development time, and other valuable features and generality provided by the conic Solver, such as the <code>COSMO.jl</code>. 
                    </li>
                </ul>
            </p>
            <h3>By Passing the Problem all Together</h3>
            <p>
                This leads to the next part of the article. There is another state of the art solution that is easier to implement, and when implemented in julia, it <b> produces up to 500 times speed up compare to the <code>JuMP.jl + COSMO.jl</code> approach </b>. It's called the Proximal Gradient Descend Method.
            </p>
        </section>
        <hr>
        <section id="pgd">
            <h2>Proximal Gradient Descend</h2>
            <p>
                It's gradient descend that can handle the sum of 2 convex functions $g(x) + h(x)$ where $h(x)$ is non-smooth but convex. In this section we will derive everything we need for th Lasso Path problem cause it's not that hard to be honest. In addition, we will make the assumption that the function $g(x)$ is smooth convex, and it's so smooth that it can be bounded above by the quadratic with convexity of $\beta$, in the sense that: 
                $$
                    g(y) \le (y - x)^T\nabla g(x) + \frac{\beta}{2}\Vert y - x \Vert_2^2 
                $$
                Which then we have: 
                $$
                    g(y) + h(y) \le g(x) + (y - x)^T\nabla g(x) + \frac{\beta}{2}\Vert y - x\Vert_2^2 + h(y)
                $$

            </p>
            <h3 id="the-proximal-gradient">Defintion: The Proximal Gradient</h3>
            <p>
                <blockquote>
                    $$
                    \underset{h, t}{\text{prox}}(
                        z
                    ) = 
                    \arg\min_x \left\lbrace
                        \frac{1}{2t}
                        \left\Vert
                            x - z
                        \right\Vert^2
                        + 
                        h(x)
                    \right\rbrace
                    $$
                </blockquote>
            </p>
            <p>
                The proximal operator find the closest point, it take the non-smooth function $h(x)$ into considerations. The proximal operator is paramaterized by a parameter $t$. 
            </p>
            <h3 id="claim-1">Claim 1</h3>

            <p>
                To get on step for proximal gradient descend, we need to minmize this, which will cause a proximal operator to arise. 
                <blockquote>
                    $$
                    \arg\min_y \left\lbrace
                        g(x) + \nabla g(x)^T(y - x) + \frac{\beta}{2}
                        \Vert y - x\Vert + h(y)
                    \right\rbrace \tag{1}
                    $$                    
                </blockquote>
            </p>
            <p>
                In which, we are trying to minimize the parabolic curve with convexity $\beta$, while also keeping the non-smooth function on the right hand side. 
            </p>
            <h3>proof</h3>
            <p>
                $$
                \begin{aligned}
                    & g(x) + \nabla g(x)^T(y - x) + \frac{\beta}{2} \Vert y - x\Vert^2
                    \\
                    =&
                    g(x) + \nabla g(x)^T x - \nabla g(x)^T x + 
                    \frac{\beta}{2}\Vert x\Vert^2 + \frac{\beta}{2} \Vert x\Vert^2
                    -\beta y^T x + h(y)
                    \\
                    =& 
                    \left(
                        g(x) - \nabla g(x)^Tx + \frac{\beta }{2} \Vert x\Vert^2
                    \right) + 
                    \underbrace{\left(
                        \nabla g(x) - \beta x
                    \right)^T y + \frac{\beta}{2} \Vert y\Vert + h(y)}_{\text{Optimize This!}}
                \end{aligned}\tag{2}
                $$
            </p>
            <p>
                We choose to optimize the part where, parameter $y$ is involved. 
            </p>
            <p>
                $$
                \begin{aligned}
                    & \arg\min_y \left\lbrace
                    g(x) + \nabla g(x)^T(y - x) + \frac{\beta}{2} \Vert y - x\Vert^2
                    \right\rbrace
                    \\
                    =&
                    \arg\min_y \left\lbrace
                        \underbrace{\left(
                            \nabla g(x) - \beta x
                        \right)^T y + \frac{\beta}{2} \Vert y\Vert^2}_{\text{the smooth part}} + h(y)
                    \right\rbrace 
                \end{aligned}\tag{3}
                $$
            </p>
            <p>
                Only the smooth part is able to be optimized in this case, now let's focus on that part. 
            </p>
            <p>
                $$
                \begin{aligned}
                    \underbrace{\left(\nabla g(x) - \beta x\right)^T}_{b} y + \underbrace{\frac{\beta}{2}}_{a} \Vert y\Vert =&
                    b^Ty + a \Vert y\Vert^2
                    \\
                    =&
                    a \left(
                        \frac{b^T}{a}y + \Vert y\Vert^2
                    \right)
                    \\
                    =& 
                    a \left(
                        \frac{b^T}{a}y + \Vert y\Vert^2 + 
                        \left\Vert\frac{b^T}{2a}\right\Vert^2
                        - 
                        \left\Vert
                             \frac{b^T}{2a}
                        \right\Vert^2
                    \right)
                    \\
                    =& 
                    a \left(
                        \underbrace{\left\Vert
                              y + \frac{b}{2a}
                        \right\Vert^2}_{\text{Optimize This part}}
                        -
                        a \left\Vert
                             \frac{b^T}{2a}
                        \right\Vert^2
                    \right)
                    \\
                    \left\Vert
                        \frac{b}{2a} + y
                    \right\Vert^2
                    =& 
                    \left\Vert
                         \frac{\nabla g(x) - \beta x}{\beta} + y
                    \right\Vert^2 
                    \\
                    =& 
                    \left\Vert
                        y - \left(x - \frac{\nabla g(x)}{\beta}\right)
                    \right\Vert^2
                \end{aligned}\tag{4}
                $$
            </p>
            <p>
                What we did is called: Completing the square, just some grade school math, and then, we susbtitute it back to the original expression and have: 
                $$
                \begin{aligned}
                    =& 
                    \arg\min_y\left\lbrace
                        \left(
                            \nabla g(x) - \beta x
                        \right)^T y + \frac{\beta}{2} \Vert y\Vert
                        + h(x)
                    \right\rbrace 
                    \\
                    =& 
                    \arg\min_y\left\lbrace
                        \frac{\beta}{2}\left\Vert
                        y - \left(x - \frac{\nabla g(x)}{\beta}\right)
                    \right\Vert^2 + h(x)
                    \right\rbrace
                    \\
                    =& 
                    \underset{h, t}{\text{prox}} \left(x - \frac{\nabla g(x)}{\beta}\right) \text{ where: } t= \frac{1}{\beta}
                \end{aligned}\tag{5}
                $$
            </p>
            <p>
                Therefore, the minimization along the parabolic with the non-smooth convex function is the same as solving the above proximal operator. The proximal opeartor should be anchored at $x = x^{(k)}$, the guess from the previous iteration, while solving for the next step in the gradient descend. <b>This can be generalized to ANY step sizes.</b>  
            </p>
            <h3 id="formulation-of-the-algorithm">Formulation of the Algorithm</h3>
            <p>
                In our case, we will be using Nesterov Accelerated Gradient, with the Proximal Operator, giving us:
                <blockquote>
                    $$
                    \begin{aligned}
                        & x_{k + 1} = \underset{1/\beta, h}{\text{prox}}\left(
                            y_k - \frac{1}{\beta}\nabla f(y_k)
                        \right)
                        \\
                        & t_{k + 1} = \frac{1 + \sqrt{1 + 4t_k^2}}{2}
                        \\
                        & y_{k + 1} = x_{k + 1} + \frac{t_k - 1}{t_{k + 1}}(x_{k + 1} - x_k)
                    \end{aligned}
                    $$
                </blockquote>
            </p>
            <h3 id="proximal-operator-for-the-l1-regularization">Proximal Operator for the L1-Reguarlization</h3>
            <p> The proximal solution for the nonsmooth part is the same as this problem: </p>
            <p>
                $$
                \arg\min_x \left\lbrace
                    \frac{1}{2t} \Vert x - y \Vert^2 + \lambda\Vert x\Vert_1
                \right\rbrace
                $$
            </p>
            
                    
            <p> Our objective is to solve this. Consider this: </p>
            
            <p>
            $$
            \begin{aligned}
                x^+=& \arg \min_x \left\lbrace
                    \frac{1}{2t} \Vert x - y \Vert^2 + \lambda\Vert x\Vert_1
                \right\rbrace 
                \\
                =& 
                \arg\min_{x_1, x_2 \cdots x_n} \left\lbrace
                \sum_{i = 1}^{n}\left(
                        \frac{1}{2t} (x_i - y_i)^2 + \lambda |x_i|
                    \right)
                \right\rbrace
                \\
                \implies 
                x^+_i =& \arg\min_{x_i}\left\lbrace
                    \frac{1}{2t}(x_i - y_i)^2 + \lambda |x_i|
                \right\rbrace
            \end{aligned}
            $$
            </p>
                    
            <p> We need to solve for each element of $x_i$, and will need to discuss them by cases because $|x_i|$ is not smooth. </p>
                    
            <p> Assuming that $x_i \neq 0$, then: </p>
                
            <p>
                $$
                \begin{aligned}
                    \bar{\partial}\left[
                        \frac{1}{2t}(x_i - y_i)^2 + \lambda |x_i|
                    \right] &= 0
                    \\
                    \frac{1}{t}(x_i - y_i) \pm \lambda &= 0
                    \\
                    x_i - y_i \pm \lambda t &= 0
                    \\
                    x_i &= y_i \mp \lambda t
                \end{aligned}
                $$
            </p>
            
                    
            <p> Take notice that, when </p>
            
            <p>
                $$
                \begin{aligned}
                    x_i \ge 0 &\implies y_i - \lambda t \ge 0 \implies y_i \ge \lambda t
                    \\
                    x_i \le 0 &\implies y_i + \lambda t \le 0 \implies y_i \le -\lambda t
                \end{aligned}
                $$
              
            </p>
                  
            <p> And if $x_i = 0$, then we will need subgradient, and we have: </p>
                    
                    
            <p>
                $$
                \begin{aligned}
                    \partial\left[
                        \frac{1}{2t}(x_i - y_i)^2 + \lambda |x_i|
                    \right] &\ni 0
                    \\
                    \frac{1}{t}(x_i - y_i) + [-\lambda, \lambda] &\ni 0
                    \\
                    x_i - y_i + [-t \lambda, t \lambda] &\ni 0
                    \\
                     \text{set }x_i &= 0
                    \\
                    y_i \in [-t \lambda, t\lambda]
                \end{aligned}
                $$
            </p>
            
                    
            <p> So basically let: </p>
                  
            <p>
                $$
                f(y) = \begin{cases}
                    y - \lambda t  & y \ge \lambda t
                    \\
                    y + \lambda t & y \le -\lambda t
                    \\
                    0 & y \in [-t \lambda, t\lambda]
                \end{cases}
                $$ 
            </p>
                    
            <p> Then</p>
            <p>
                $$
                x^+ = f \bullet(y)
                $$
            </p>      
            
                    
            <p> Where $\bullet$ saying that, the function has been vectorized on each element of the input vector. </p>
                    
            <p> 
                up to this point, we have solved the proximal operator for all of the L1 Norm Regularized regression problem, the function vectorized function $f$ is the solution. 
            </p>

        </section>
        <hr>
        <section id="julia">
            <h2>Julia Programming Language For speed: The Home-made Proximal Gradient Descned Method</h2>
            <p>
                The julia programming language supports JIT interpreter, it it compiles to code using LLVM. It uses dynamic dispatch and both static and dynamic typing. 
            </p>
            <p>
                The key to speed is to restrict the type for the functions we use, so that when the function is called, it will only be compiled for one specific type by the JIT (Which itself made the function, giving it shorter assembly code), and then it's never compiled again, results in faster execution time.
            </p>
            <p>
                Here is the code implementations of what we talked about: 
            </p>
            <p>
                <pre>
                    <code class="julia">
mutable struct ProximalGradient
    """
        Dynamical, functional, keeps references to detailed implementation 
        functions. 
    """

    g::Function 
    gradient::Function
    prox::Function
    h::Function
    
    β::Float64  # For beta convex function. 
    tol::Float64
    maxItr::Int64 
    solutionDim::Tuple

    function ProximalGradient(tol::Float64, maxItr::Int64)
        this = new()
        this.tol = tol
        this.maxItr = maxItr
        return this
    end

end


function OptimizeProximalGradient(
        this::ProximalGradient, 
        warm_start::Union{Array, Nothing}=nothing
    )::Matrix
    """
        Implement FISTA, Accelerated Proximal Gradient, copied from my HW. 
    """
    tol = this.tol
    max_itr = this.maxItr
    
    if warm_start === nothing
        warm_start = zeros(this.solutionDim)
    end

    x  = warm_start
    Δx = Inf
    y  = x
    t  = 1
    ∇f = this.gradient(y)
    δ  = 1/this.β # stepsize
    xNew = similar(x)
    yNew = similar(y)

    while Δx >= tol && max_itr >= 1
        xNew = x - δ*∇f
        this.prox(xNew, δ)
        tNew = (1 + sqrt(1 + 4t^2))/2
        yNew .= xNew
        yNew += ((t - 1)/tNew)*(xNew - x)

        ∇f .= this.gradient(yNew)
        Δx = norm(xNew - x, 1)/norm(x, 1)
    
        t = tNew
        x = xNew
        y = yNew
        
        max_itr -= 1
    end
    if max_itr == 0
        println()
        Warn("Maximal iteration $(this.maxItr) reached for Proximal gradient. ")
        @assert norm(x, Inf) != Inf "Solution is infinite, something blowed up."
        Warn("It will be bumped up by 200% more and then we try again.")
        this.maxItr = 2*this.maxItr
        return OptimizeProximalGradient(this, warm_start)
    end

    return x

end



### ----------------------------------------------------------------------------

### ============================================================================
### Lasso Proximal Operator
### ============================================================================

function L1LassoProximal!(y::Union{Vector, Matrix}, t::Float64, λ::Float64)
    """
        Textbook definition of the L1 Lasso proximal operator. 
    """

    return map!(y, y) do yi
        if yi > λ*t
            return yi - λ*t
        elseif yi < -λ*t
            return yi + λ*t
        else
            return 0
        end
    end

end


### ----------------------------------------------------------------------------

### ============================================================================
### The 2 Norm loss function with L1 regularization
### ============================================================================


function BuildPG2NormLasso(
        A::Matrix, 
        b::Matrix, 
        λ::Float64, 
        tol::Float64=1e-5,
        max_itr::Int64=1000
    )::ProximalGradient
    """
        Given matrix A, vector b, and λ the regularization perameter, 
        this will build an instance of ProximalGradient. 

        * Use Lambdas to capture variables references 
        * A factory methods
        ! Pass copy of matrices will be better. 
    """

    @assert size(b, 2) == 1 "Expect vector b in the"*
    " shape of (n, 1) but it turns out to be $(size(b))"
    @assert size(b, 1) == size(A, 1) "Expect the size of the matrix to match "*
    "the vector but it turns out to be A is in $(size(A)), and b is $(size(b))"
    ATA = A'*A
    ATb = A'*b
    β = 4*opnorm(ATA)  # convexity from spectral norm. 
    t = 1/β
    f(x) = norm(A*x - b)^2
    dg(x) = 2(ATA*x - ATb)
    
    # ======= build ==================================
    proxVectorized(y, t) = L1LassoProximal!(y, t, λ)
    this = ProximalGradient(tol, max_itr)
    this.g = f
    this.gradient = dg
    this.prox = proxVectorized
    this.β = β
    this.solutionDim = (size(A, 2), size(b, 2))
    this.h = (x) -> norm(x, 1)

    return this
end

function ChangeProximalGradientLassoλ!(this::ProximalGradient, λ::Float64)
    """
        Change the proximal operator for a different λ
    """
    this.prox = (y, t) -> L1LassoProximal!(y, t, λ)
end
                    </code>
                </pre>
            </p>
            <h3>Julia Package</h3>
            <p>
                The whole project is developed as a julia package, the repos for the package is <a href="https://github.com/iluvjava/LassoPath.jl"><code>LassoPath.jl</code></a>. 
            </p>

        </section>
        <hr>
        <section id="res">
            <h2>Results: Impressive</h2>
            <p>
                Results are carried out with the crime dataset from UCI machine learning dataset. The link to the data set is <a href="https://archive.ics.uci.edu/ml/datasets/communities+and+crime">here</a>. 
            </p>
            <p>
                Under around 5 seconds, on my machine with a Ryzen 4900HS, the equivalent job takes 550 second, with compromised precision, and less $\lambda$ values for the search when using quadratic programming with the <code>JuMP.jl</code> with <code>COSMO.jl</code>. Here is the lasso plot graph for the data: 
            </p>
            <p>
                <img src="./lasso_proximal.png" alt="lasso path of the data set">
            </p>
        </section>
        <hr>
        <section id="epilogue">
            <h2>Epilogue</h2>
            <p>
                Here, we tie up some loose end, and I will point out further works, and insights for this project. 
            </p>
            <h3>Everything I did is Very Basic and there is much MORE</h3>
            <p>
                Here are more ideas: 
                <ol>
                    <li>
                        There is a way to get exactly when the value of $\lambda$ causes the solution to "bifurcates" which uses strong duality, which will give use precisely the order decay for the weights. But it requires a close look into the KTT of the problem. 
                    </li>    
                    <li>
                        The quadratic programming formulation also has a dual, but it will be different from the dual of the non-smooth formulations, a look into this should creates more insights into the problem, more specifically how to formulate constraints involving ordering, say, $\Vert \bullet \Vert_{\infty}$ as a constraint. You should know how to do it it if you are taught on linear programming and modeling. 
                    </li>
                    <li>
                        My Implementation of the proximal algorithm didn't exploit the dual formulations, which can be helpful because sometimes the dual is easier to solve. 
                    </li>
                    <li>
                        This proximal gradient is applicable to all other types of convex smooth objectives, such as cross entropy, logit loss, huber loss. 
                    </li>
                    <li>
                        For non-smooth objective function, we need Chambollo Pock (Uses operator theory and a lot of fancy stuff), OR, just use line search for the gradient which will make the algorithm much slower for larger problem. 
                    </li>
                    <li>
                        Alternate coordinate descend can also solve this, but it will be much slower. 
                    </li>
                </ol>
            </p>
            <h3>Exercise for the Reader</h3>

            <p>
                <blockquote>
                    Prove for the value claim for $\lambda_{\max}$, which is the minimum $\lambda$ that set all the weights to zero. Hint: Use Subgradient. 
                </blockquote>
            </p>
        </section>
        <div class="footer-padding">
        </div>
    </div>
    <footer>
        <!--For footer context botton-->
        <div class="fixed-bottom hide" id="footer-display">
            <div class="row align-items-center" id="footer-display-inner">
                <div class="col text-right">
                    <button type="button" class="btn btn-primary toggle-btn">Toggle Big Printout</button>
                </div>
            </div>
        </div>
        <!--Ends-->
    </footer>
</body>
</html>