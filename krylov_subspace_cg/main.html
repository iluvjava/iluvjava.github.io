<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">
    <!--Edit Here-->
    <title>Conjugate Gradient with Krylov Subspace</title>
    <!--Open graph protocol EDIT HERE-->
    <meta property="og:title" content="Conjugate Gradient with Krylov Subspace">
    <meta property="og:type" content="article">
    <meta property="og:description"
    content="A new derivation of the same conjugate gradient algorithm using minimization on energy norm of the error on the krylov subspace.">
    <meta property="og:image" content="https://iluvjava.github.io/krylov_subspace_cg/energy_norm.png" >
    <!--END-->
    <!--Edit Here -->
    <!-- ForTwitter-->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Conjugate Gradient with Krylov Subspace" />
    <meta name="twitter:description" content="A new derivation of the same conjugate gradient algorithm using minimization on energy norm of the error on the krylov subspace.">
    <!---->
    <!--For Bootstrap style-->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
        integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!--Bootstrap end-->

    <!--MyStyle-->
    <link rel="stylesheet" href="mystyle.css">
    <!--End-->

    <!--For plotting data and Adpating Webpage to mobile devices-->
    <script src="client_scripts_general.js"></script>
    <!-- End -->

    <!--Plotly-->
    <script src="https://cdn.plot.ly/plotly-1.2.0.min.js"></script>
    <!--FIX VERSION, new UPDATE has problem-->
    <!--END-->

    <!--JS hilight for the code blocks in the web: -->
    <link rel="stylesheet" href="../assets/client_scripts/syntax_highlight_pack/styles/a11y-light.css">
    <script src="../assets/client_scripts/syntax_highlight_pack/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <!--END-->
    <!--For Katex -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js" integrity="sha384-YNHdsYkH6gMx9y3mRkmcJ2mFUjTd0qNQQvY9VYZgQd7DcN7env35GzlmFaZ23JGp" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
            // customised options
            // • auto-render specific keys, e.g.:
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false},
                {left: '\\(', right: '\\)', display: false},
                {left: '\\[', right: '\\]', display: true}
            ],
            // • rendering keys, e.g.:
            throwOnError : false
            });
        });
    </script>
    <!--END-->
</head>
<body>
    <nav class="navbar sticky-top">
        <div class="container-fluid">
            <div class="navbar-header">
                <!-- Edit here -->
                <a class="navbar-brand" href="../index.html">My WebPages</a>
            </div>
            <botton>
                <span class="nav-hamburger-menu-icon hide"><img src="../assets/menu_hamburger.png" alt="menu"></span>
            </botton>
            <div class="collapse navbar-collapse">
                <ul>
                    <!--Section links. Edit Here -->
                    <li><a href="#intro">Intro</a></li>    
                    
                </ul>
            </div>
        </div>
    </nav>
    <div id="side-bar">
        <ul>
            <!--Section links. Edit Here-->
            <li><a href="#intro">Intro</a></li>
            
        </ul>
    </div>
    <div id="main-content">
        <section id="intro">
            <h2>Intro</h2>
            <p>
                Last formulation of the conjugate gradient algorithm failed to reveal more than what conjugate gradient is, as if, it's just a matter of "do things in this way then we have the right thing."
                
            </p>
            <p>
                But why? Why is it that the energy norm monotonically decreases? Why is steepest gradient along conjugate direction works? 
            </p>
            <p>
                <h3>Prerequisite</h3>
                The previous derivation of the conjugate gradient: <a href="../conjugate_gradient/main.html">here</a>. 
            </p>
            <h3>Major Ideas</h3>
            <blockquote>
                A new formulation of the conjugate gradient algorithm is what this page is about. We will use the idea of Krylov Subspace to derive the Conjugate gradient algorithm. And the following will be addressed: 
                <ol>
                    <li>
                        Minimizing the energy norm of error vector $e^{(k)}$ under the Krylov Subspace $\mathcal{K}_k$ at each iteration will give us the conjugate gradient algorithm. 
                    </li>
                    <li>
                        Formulate the algorithm and prove that it's exactly the same as the previous version, hence, explaining the magics behind the first formulation. 
                    </li>
                    <li>
                        There is a connection between CG and lancosz algorithm. 
                    </li>
                </ol>
            </blockquote>
            <h3>Why Krylov Subpace</h3>
            <p>
                Minimization using Krylov Subspace defines a class of Iterative Methods for linear system, and Conjugate Gradient is just one of them. Nearly all other iterative has some kind of connection to minimization in Krylov Subpsace. 
            </p>
            <h3>List of Quantities and Assumptions</h3>
            <p>
                <ol>
                    <li>$\langle \bullet \rangle$ means that span of a set of vectors that are in the angle bracket. </li>
                    <li>$\langle u,v \rangle$ is the inner product of 2 vectors</li>
                    <li>$\langle u, v \rangle_A$ is the inner product of 2 vectors under the A (Symmetric Positive Definite)SPD matrix, basically: $u^TAv$</li>
                    <li>$Ax^+ = b$ the equation and it's soltuion is $x^+$, regarded as the optimal value $x^+$. </li>
                    <li>$e^{(k)} = x^{(k)} - x^+$ The error vector, when the solution $x^{(k)}$ is converging. </li>
                    <li>$r^{(k)} = b - Ax^{(k)}$, the residual vector, how far we are from the correct solution on the output space of matrix $A$. </li>
                    <li>$Ae^{(k)} = - r^{(k)}$, The relation between the input space error vector and the output space residual vector. </li>
                    <li>$\Vert x\Vert_A$ is the Energy norm of a vector $\Vert x\Vert_A^2 = x^TAx$, where $A$ is assumed to be SPD. </li>
                    <li>$u\perp_A v$, vector $u, v$ are A-orthogonal to each other, it's equivalent to $\langle u, v \rangle_A$</li>
                </ol>
            <ol>
                Assume that matrix $A$ is SPD, squared. 
            </ol>
        
        </p>
            
        </section>
        <hr>
        <section id="krylov">
            <h2>Krylov Subpace</h2>
            Definition: 
            $$
            \langle \mathcal{K}_k\rangle = \langle b, Ab, A^2b, \cdots A^{k}b \rangle
            $$
            <h3>Property 1</h3>
            $$
                \langle \mathcal{K}_0\rangle\subseteq
                \langle \mathcal{K}_1 \rangle \subseteq  
                \langle \mathcal{K}_2 \rangle \subseteq 
                \langle \mathcal{K}_3 \rangle \cdots 
            $$
            <p>It's trivial to justify it.</p>
            <h3>Property 2</h3>
            <p>
                $\mathcal{K}_n$ spans $\mathbb{R}^n$ if $A$ is invertible. 
            </p>
            <h4>Justification</h4>
            <p>
                This is the characteristic polynomials of a matrix A, it's monic with $c_{n} = 1$ and $c_0 = (-1)^n|A|$. 
                $$
                    p(\lambda) = \sum_{i = 0}^{n}
                    c_0 \lambda^i = |A - \lambda I|
                $$
                Using the <a href="https://en.wikipedia.org/wiki/Cayley%E2%80%93Hamilton_theorem">Cayley-Hamilton Theorem</a> , putting the matrix $A$ into the polynoial will result in the zero matrix. Therefore, The inverse matrix $A^{-1}$ can be represented using the coefficients of the characteristic polynomial.
                $$
                \begin{aligned}
                    \mathbf{0} &= 
                        c_0I_n + c_1A + c_2A^2 \cdots c_nA^n
                    \\
                    -c_0I_n &=  
                    c_1A + c_2A^2 \cdots c_nA^n
                    \\\implies
                    A^{- 1} &= \frac{c_1}{c_0}I_n + \frac{c_2}{c_0}A \cdots \frac{c_n}{c_0}A^{n - 1}
                    \\\implies
                    b &= \frac{c_1}{c_0}Ab + \frac{c_2}{c_0}A^2b \cdots \frac{c_n}{c_0}A^nb
                \end{aligned}
                $$
                Apply vector $b$ on both side of the equation, the krylov subspace spans the range of the inverse matrix. Which means that we can solve for any vector using the Krylov Subspace, if $A$ can be inverted. 
            </p>
            <p>
                The proof for Cayleys-Hamilton Theorem is skipped, under the case where $A$ is SPD, it's very easy, but other the general case, I don't understand the proof for it yet. 
            </p>
            
        </section>
        <hr>
        <section id="cg_theory">
            <h2>Conjugate Gradient: Theory</h2>
            <h3>Claim 1</h3>
            <p>
                <blockquote>
                    The conjugate graidient algorithm minimizes the energy norm of the error vector at each iterations of the algorithm, which is to say: 
                    $$
                    \begin{aligned}
                        x^{(k + 1)} &= \arg\min_{x\in \langle K_{k + 1}\rangle} 
                            \Vert x - x^+\Vert_A^2
                    \end{aligned} \quad  \text{where: } Ax^+ = b
                    $$
                    If we do this for every $x^{k}$ then we have the conjugate gradient algorithm. 
                </blockquote>
            </p>
            <h3>A-Orthogonal Directions</h3>
            <p>
                2 Vectors are said to be A-Orthogonal if $u^TAv = 0$. This is previously referred to as being conjugate vectors. The reason as to why conjugate vector is used is because it comes handy with the energy norm, it's especially good if we can find a set of conjugate vectors that are all A-Orthogonal. 
            </p>
            <h3>Inductive Hypothesis</h3>
            <p>
                <blockquote>
                    $$
                    \begin{aligned}
                    \langle  d_0\rangle  &= \langle  b\rangle = \langle \mathcal{K}_0 \rangle
                    \\
                    \langle d_0, d_1\rangle &= \langle b, Ab\rangle = \langle \mathcal{K}_1 \rangle
                    \\
                    \langle d_0, d_1, \cdots d_{k - 1}\rangle &= \langle b, Ab, \cdots A^{k - 1}b \rangle = \langle \mathcal{K}_{k - 1} \rangle
                    \end{aligned}
                    $$
                    Inductively, let's also assume that: 
                    $$
                    x^{(k)} \in \langle \mathcal{K}_k \rangle
                    $$
                    And there exists a set of $n$ A-Orthogonal vector $d_i$ that spans $\mathcal{K}_n$ eventually. Recall that, this is possible through the process of <a href="../conjugate_gradient/main.html#gs-conjugation">Gram Schimdtz Conjugation</a> from the previous section. It can be used to make A-Orthogonal vectors using the Evolving Krylov Subspace. Therefore, this assumption is legit. 
                </blockquote>

            </p>
            <h3>Setting Up Some Variables</h3>
            <p>
                By the assumption that $x^{(k)} \in \mathcal{K}_k$, we know that $x^{(k)}\in \langle d_0, d_1, \cdots d_{k - 1}\rangle$, giving us: 

                $$
                \begin{aligned}
                    x^{(0)} &= \sum_{j = 0}^{n-1} a^{(0)}_j d_j
                    \\
                    x^{(k)} &= \sum_{j = 0}^{k - 1} a^{(k)}_j d_j
                    \\
                    x^{+} &= \sum_{j = 0}^{n - 1} a_j^+d_j
                \end{aligned}\tag{1}
                $$
                            
                Note: $x^{(0)}, x^+$ need to be expressed using all the conjugate, if we assume that the system is, solvable, it's solvable only if $b\in \langle\mathcal{K}_j\rangle$, $1 \le j \le n$. 
                <br>            
                To verify claim 1, we need to minimize the energy norm of $x$ under the subspace $\langle \mathcal{K}_{k + 1} \rangle$. 
                <br>
                Which them means the statement we consider in **claim 1** would be 
                <br>
                $$
                \begin{aligned}
                    x^{(k+ 1)} &= \arg\min_{x\in \mathcal{K}_{k + 1}} \Vert x - x^+\Vert_A^2
                    \\
                    \underset{[2]}{\implies}\text{let: } x &= \sum_{j = 0}^{k-1} a_j d_j 
                    \\
                    x^{(k+ 1)} &= \arg\min_{x\in \mathcal{K}_{k + 1}} 
                    \left\Vert
                        \underbrace{\sum_{j = 0}^{k} (a_j - a_j^+)d_j}_{\in \mathcal{K}_{k + 1}}
                        + 
                        \sum_{j = k}^{n - 1} a_j^+ d_j
                    \right\Vert_A^2
                    \\
                    \underset{[1]}{\implies} x^{(k+ 1)} &= \sum_{j = 0}^{k}a_j^+d_j \in 
                    \langle  \mathcal{K}_{k + 1}\rangle
                \end{aligned}\tag{2}
                $$
                <br>
                [1]: Take notice that, because $d_j$ vector is A-Ortho, therefore, it's directly:           
                $$
                    \sum_{j = 0}^{k}(a_j - a_j^+)^2 - \sum_{j = k + 1}^{n - 1}(a_j^+)^2
                $$
                We are implicitly using the PSD property of the matrix $A$ here. 
                <br>
                [2]: By the fact that $x\in \mathcal{K}_{k + 1}$, the subscript for the denoted minimizer. However, because $a_j$ where $1 \le j \le k$ corresponds to components that spans $\mathcal{K}_{k + 1}$, the minimization problem is just directly setting the conjugate vector to zero. 
            </p>
            <h3>Readers Please Observe</h3>
            <p>
                The inductive assumtpion $x\in \langle \mathcal{K}_k \rangle$ holds true. 
            </p>
            <p>
                We found an expression for $x^{(k + 1)}$ from it, which is going to be the next guess produced by the conjugate gradient algorithm. If this is 
                iteratively applied, then each time we only determine the component (the projection of $x^+$ onto) $d_k$ for vector $x^{(k + 1)}$. In this case, we jump right to the vector $x^{(k + 1)}$ directly. 
            </p>
            <p>
                Please compare the above formulation of Krylov Subspace to <a href="../conjugate_gradient/main.html#claim-1">claim 1</a> in previous discussion of Conjugate Gradient. 
            </p>
        </section>
        <hr>
        <section id="consequence">
            <h2>Other Consequences of the Theory</h2>
            <p>
                There are 3 corollaries that come with what I just did. It's some of the same details from previous discussion, but this time it's proved using Krylov Subspace. Which terms out to be more beautiful. 
            </p>
            <h3>Claim 1 Corollary 1</h3>
            <p>
                <blockquote>
                    $e^{(k)} \perp_A \langle \mathcal{K}_{k}\rangle$
                    The error vector at the $k$ th step is the orthogonal to the Krylov Subspace $\mathcal{K}_k$, because the minimization process removes the components represented by $d_i$ for $\mathcal{K}_{k}$
                </blockquote>            
                This is a direct consequence of how the optimization problem is phrased. No proof will be given here. 
            </p>
            <h3>Claim 1 Corollary 2</h3>
            <p>
                <blockquote>
                    $r^{(k)}\notin \langle \mathcal{K}_{k - 1} \rangle$, but $r^{(k)}\in \langle \mathcal{K}_{k} \rangle$, which them means that $r^{(k + 1)}\notin \langle \mathcal{K}_{k} \rangle$ then $\langle r^{(k + 1)}, r^{(k)}  \rangle = 0$
                </blockquote>
            </p>
            <h4>Claim 1 Cororllary 2 Proof</h4>
            <p>
                Consider the inductive hypothesis and the variables that we setup for minimization on the Krylov Subspace. 
                $$
                \begin{aligned}
                    x^{(k)} &=  \sum_{j = 0}^{k - 1} a_j^{+} d_j \in 
                    \langle \mathcal{K}_k \rangle
                    \\
                    b = Ax^+  \implies  b &= \sum_{j = 0}^{n - 1} a^+_jAd_j
                    \\
                    \implies
                    b - Ax^{(k)} &= - \sum_{j = k}^{n - 1}a_j^{+}Ad_j
                    \\
                    \implies 
                    \forall 0 \le i \le k - 1 & \quad 
                    \left\langle  
                        \sum_{j = k}^{n}
                        a_j^+Ad_j,
                        d_i
                    \right\rangle = 0
                    \\
                    \implies
                    r^{(k)} & \notin \langle d_0, d_1, \cdots d_{k - 1} \rangle
                    \\
                    \implies 
                    r^{(k)} & \notin \langle \mathcal{K}_{k - 1} \rangle
                \end{aligned}
                \tag{3}
                $$
            </p>
            <h3>Claim 1 Corollary 3</h3>
            
        </section>
        <div class="footer-padding">
        </div>
    </div>
    <footer>
        <!--For footer context botton-->
        <div class="fixed-bottom hide" id="footer-display">
            <div class="row align-items-center" id="footer-display-inner">
                <div class="col text-right">
                    <button type="button" class="btn btn-primary toggle-btn">Toggle Big Printout</button>
                </div>
            </div>
        </div>
        <!--Ends-->
    </footer>
</body>
</html>