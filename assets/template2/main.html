<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">
    <title>L1, L2 Norm as Loss Function</title>

    <!--For Bootstrap style-->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
        integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!--Bootstrap end-->

    <!--MyStyle-->
    <link rel="stylesheet" href="mystyle.css">
    <!--End-->

    <!--For plotting data, Adpating Webpage to mobile devices-->
    <script src="client_scripts_general.js"></script>
    <!-- End -->

    <!--Plotly.jl-->
    <script src="https://cdn.plot.ly/plotly-1.2.0.min.js"></script>
    <!--FIX VERSION, new UPDATE has problem-->
    <!--END-->

    <!--JS hilight for the code blocks in the web: -->
    <link rel="stylesheet" href="../assets/client_scripts/syntax_highlight_pack/styles/a11y-light.css">
    <script src="../assets/client_scripts/syntax_highlight_pack/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <!--END-->
    
    <!--For Katex -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js" integrity="sha384-YNHdsYkH6gMx9y3mRkmcJ2mFUjTd0qNQQvY9VYZgQd7DcN7env35GzlmFaZ23JGp" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
            // customised options
            // • auto-render specific keys, e.g.:
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false},
                {left: '\\(', right: '\\)', display: false},
                {left: '\\[', right: '\\]', display: true}
            ],
            // • rendering keys, e.g.:
            throwOnError : false
            });
        });
    </script>
    <!--END-->
</head>
<body>
    <nav class="navbar sticky-top">
        <div class="container-fluid">
            <div class="navbar-header">
                <!-- Edit here -->
                <a class="navbar-brand" href="../index.html">My WebPages</a>
            </div>
            <botton>
                <span class="nav-hamburger-menu-icon hide"><img src="../assets/menu_hamburger.png" alt="menu"></span>
            </botton>
            <div class="collapse navbar-collapse">
                <ul>
                    <!--Section links. Edit Here -->
                    <li><a href="#intro">Intro</a></li>    
                    <li><a href="#lp">Phrasing it as LP</a></li>
                    <li><a href="#cvxopt">CVXOPT</a></li> 
                    <li><a href="#bootstrap">Bootstrap and Confidence Interval</a></li>
                    <li><a href="#results">Results</a></li>
                </ul>
            </div>
        </div>
    </nav>
    <div id="side-bar">
        <ul>
            <!--Section links. Edit Here-->
            <li><a href="#intro">Intro</a></li>
            <li><a href="#lp">Phrasing it as LP</a></li>
            <li><a href="#cvxopt">CVXOPT</a></li>
            <li><a href="#bootstrap">Bootstrap and Confidence Interval</a></li>
            <li><a href="#results">Results</a></li>
        </ul>
    </div>
    <div id="main-content">
        <section id="intro">
            <h2>
                Introduction
            </h2>
            <p>We are interested in comparing and visualizing the difference between using L1 norm as loss function and L2 norm as loss function for simple regression tasks.</p>
            <p>It's equivalent to solving the following optimization problem: </p>
            <p>
$$
\alpha^+ = \arg\min_x 
    \Vert Ax - b\Vert_1
$$
            </p>
            <ul>
                <li>\(A\) is the row data matrix, or the vandermande matrix in our case because we want to use polynomial regression examples. (We want the column for ones to be the last column of the vandermonde matrix)</li>
                <li>\(x\) is the coefficients for the regression</li>
                <li>\(b\) is the regression value we are trying to fit</li>
            </ul>
            <p>And for L2 Norm Loss function, we have: </p>
            <p>
$$
\alpha^+ = \arg\min_x 
    \Vert Ax - b\Vert_2^2
$$
            </p>
            <p>
                Here is our list of topics we are gonna cover in this website
            </p>
            <ul>
                <li>Intro</li>
                <li>Solving it as Linear Programming Problem</li>
                <li>Non-Parametric Bootstrap</li>
                <li>Code</li>
                <li>Results</li>
            </ul>
            A repo with all the code involved can be viewed <a href="https://github.com/iluvjava/Serious-Python-Stuff/blob/main/L1-Norm-Boopstrap/core.py">here</a>
        </section>
        <hr>
        <section id="lp">
            <h2>Phrasing L1 Loss as a Linear Programming Problem</h2>
            <p>$$\begin{aligned}
                & \min_{x} \Vert Ax - b\Vert_1 
                \\
                & \min_{x} \left\vert \sum_{j = 1}^m a_{i, j}x_j - b_i\right\vert \quad \forall 1 \le i \le n
                \\
                & \min_{t, x} \left\lbrace -t < \sum_{i = j}^m a_{i, j}x_j - b_i < t \right\rbrace 
                \quad \forall 1 \le  i \le n
                \\
                =& \min_{v, x} \left\lbrace
                    v : -v \le Ax - b \le v
                \right\rbrace
            \end{aligned}$$</p>
            <p>
                Where \(x, v\) are vectors. 
            </p>
            <p>
                This is not the optimal way, nor the professional way of doing it, I will talk more about later. 
            </p>
            <p>There are other ways of solving this problem, somve of the alternatives includes: </p>
            <ul>
                <li>
                    Coordinate Descend
                </li>
                <li>
                    Non-smooth Optimization Algorithm, eg: FISTA. 
                </li>
                <li>Exploting the symmetric properties of the L1 constraint to speed up computing the KTT conditions, which then, speeds up the interior points algorithm. See an link to this in the next section for more information.</li>
            </ul>
        </section>
        <hr>
        <section id="cvxopt">
            <h2>CVXOPT</h2>
            <p>For solving the system, I used a thing called: CVXOPT, available <a href="https://cvxopt.org/">here</a>, it uses Interior Points method, implemented in python using BLAS, which I believe it's not that bad of a solution. It's also currebtly maintained by <b>M. Andersen and L. Vandenberghe, </b> at UCLA.</p>
            <p>CVXOPT provides a formulation of the L1 Loss function as a LP problem that exploits the symmetric structure of the matrix to improve the complexity of the algorithm. Which I didn't use. See <a href="https://cvxopt.org/examples/mlbook/l1.html">here</a> for more.</p>
            <p>Here is the code using the CVXOPT to get the coefficients for the polynomial fit on the data with the L1 Norm Loss</p>
            <pre><code class="python">
import matplotlib.pyplot as plt
import numpy as np
import cvxopt as cvx
import cvxopt.modeling as cvxm
from cvxopt import solvers
solvers.options["show_progress"] = False
solvers.options["abstol"] = 1e-15
solvers.options["reltol"] = 1e-10
solvers.options["refinement"] = 10

from tqdm import tqdm

def L1RegFit(X: np.ndarray, y:np.ndarray):
    assert X.ndim == 2 and X.shape[0] == y.shape[0]
    m, n = X.shape
    A = cvx.matrix(X)
    x = cvxm.variable(n)
    b = cvx.matrix(y)
    Epsilon = cvxm.variable(m)
    Lp = cvxm.op(cvxm.sum(Epsilon))
    Lp.addconstraint(-Epsilon <= b - A*x)
    Lp.addconstraint(b-A*x <= Epsilon)
    Lp.solve()
    return (np.array(x.value)).reshape(-1)
            </code></pre>
        </section>
        <hr>
        <section id="bootstrap">
            <h2>Boostrap For Confidence Interval</h2>
            <p>Bootstrap, it's one of the way to estimate the variance of the model, when we don't know enough about the original generative process behind the data and getting new data is impossible. Non-Parametric Bootstrap to be specific.</p>
            <p>Given training data matrix \(A\), let \(\tilde{A}\) be a list subset of matrix created by sampling rows of \(A\) with replacement, \(k\) times, which means that, it's absolutely possible, for all rows of \(\tilde{A}\) to be all the same row, but that is very unlikely when we have a lot of rows in matrix \(A\)</p>
            <ul>
                <li>\(\;B\) is the number of bootstrap we want to make. </li>
                <li>\(\;\mathbb{II}\) variable to keep track of the number of iterations for the for loop.</li>
                <li>\(\;\tilde{A}\) is a matrix whose rows are sample from the matrix A with replacment, recall that the matrix \(A\) is the vandermonde matrix of all the training pints.</li> 
                <li>\(\;Y\) constains all the rows of data predicted by all the models from the bootstrap process.</li>
            </ul>
            <h3>Bootstrap Psuedocodes</h2>
            <div class="indent-all">
                Let \(\mathbb{M}\) be a set that holds trained model from the bootstrap for loop. 
                <br>
                Let \(X\) be the Vandermond Matrix for the testing data set. 
                <br>
                for \(\mathbb{II}\) = \(1\) to \(B\)
                <div class="indent-all">
                    Make Boostrap Data \(\tilde{A}\) from matrix \(A\), resampled with replacement \(k\) times. 
                    <br>
                    Make model \(\alpha\) using \(\tilde{A}\). 
                    <br>
                    Add \(\alpha\) to \(\mathbb{M}\). 
                </div>
            </div>
            <div class="indent-all">
                Let \(\hat{Y}\) be row of data predict by all the models in \(\mathbb{M}\)
                <br>
                For \(\mathbb{M}\), \(\alpha\) in Enumerate(\(\mathbb{M}\)): 
                <div class="indent-all">
                    \(\hat{Y}\)[\(\mathbb{II}\), :] := \(X\alpha\)
                </div>
                Let \(\hat{Y}_{\text{Lower}}\) to be a row vector of the 2.5 percenttile of all columns of matrix \(\hat{Y}_{\text{Lower}}\). 
                <br>
                Let \(\hat{Y}_{\text{Upper}}\) to be a row vector of the 97.5 percenttile of all columns of matrix \(\hat{Y}_{\text{Upper}}\). 
            </div>
            <h3>Code Implementation</h3>
            <pre><code class="python">
def RegBoopStrap(
                X:np.ndarray,   # regression data
                y:np.ndarray,   # labels
                fxn:callable,   # model training function
                queries=None,   # the points to query the confident interval
                trials=None,    # number of times to make the boostrap the model
                boostrapSize:int=None,
                alpha:float=5 # confidence band interval
                ):
    """
        Performs non-parametric boopstrap for regression models
    :param X:
    :param y:
    :param fxn:
    :param queries:
    :param trials:
    :param boostrapSize:
    :return:
    """
    assert X.ndim == 2 and X.shape[0] == y.shape[0] and y.ndim == 1
    assert X.ndim == 2
    assert alpha < 50 and alpha > 0
    m, n = X.shape
    trials = X.shape[0] if trials is None else trials
    queries = X[:, 1] if queries is None else queries
    queries.sort()
    bootstrapSize = m if boostrapSize is None else boostrapSize
    assert trials > 20
    assert bootstrapSize <= m, "don't recommend bootstrap size to be larger than sample size"
    BoostrapYhat = []
    for _ in tqdm(range(trials)):
        Selection = np.random.randint(0, m, bootstrapSize)
        Coefficient = fxn(X[Selection], y[Selection])
        V = np.vander(queries, Coefficient.shape[0])
        Yhat = (V@Coefficient[:, np.newaxis]).reshape(-1)
        BoostrapYhat.append(Yhat)
    OutputMatirx = np.array(BoostrapYhat)
    ConfidenceBand = np.percentile(OutputMatirx, [alpha/2, 100 - alpha/2], axis=0)
    return ConfidenceBand
            </code></pre>
        </section>
        <hr>
        <section class="results">
            <h2>Results</h2>
            <p>The generative model we used is \(\cos(\pi x)\), on the interval (-1, 1), with constant variance of 0.01, and 20% of the data are outliers set to be at a value of exactly -3. </p>
            <p>Model with L1, L2 norm as loss function are trained, with 300 boopstraped models and \(k = n\) where \(n\) is the number of rows of matrix \(A\).</p> 
            <p>The degree of polynomials we use to fit the generative model is: 4</p>
            <p>Then, a confidence 99% confidence interval are produced, with a fine grid. </p>
            <p>This is the results</p>
            <img src="bootstrap-l1-l2.png" alt="L1, L2 Confidence Band">
            <p>Observe that the confidence band of the model with L1 norm as loss function is narrower and its centering around the ground truth generative model. Comparing to the L2 model, which is much more suspectible to the effect of the outliers below the curve. </p>
        </section>
        <div class="footer-padding">
        </div>
    </div>
    <footer>
        <!--For footer context botton-->
        <div class="fixed-bottom hide" id="footer-display">
            <div class="row align-items-center" id="footer-display-inner">
                <div class="col text-right">
                    <button type="button" class="btn btn-primary toggle-btn">Toggle Big Printout</button>
                </div>
            </div>
        </div>
        <!--Ends-->
    </footer>
</body>
</html>