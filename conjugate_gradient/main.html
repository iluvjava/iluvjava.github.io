<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">
    <title>CG and Box Deblurring</title>
    <!--Open graph protocol-->
    <meta property="og:title" content="Conjugate Gradient for Deblurring Box Blur" >
    <meta property="og:type" content="website" >
    <meta property="og:description" 
    content="A detailed look into Conjugate Gradient Method and it's applications" >
    <meta property="og:image" content="./Images/cg-box-deblurred-alto-blep.png.png" >
    
    <!--END-->
    <!--For Bootstrap style-->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
        integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!--Bootstrap end-->

    <!--MyStyle-->
    <link rel="stylesheet" href="mystyle.css">
    <!--End-->

    <!--For plotting data and Adpating Webpage to mobile devices-->
    <script src="client_scripts_general.js"></script>
    <!-- End -->

    <!--Plotly-->
    <script src="https://cdn.plot.ly/plotly-1.2.0.min.js"></script>
    <!--FIX VERSION, new UPDATE has problem-->
    <!--END-->

    <!--JS hilight for the code blocks in the web: -->
    <link rel="stylesheet" href="../assets/client_scripts/syntax_highlight_pack/styles/a11y-light.css">
    <script src="../assets/client_scripts/syntax_highlight_pack/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <!--END-->
    <!--For Katex -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js" integrity="sha384-YNHdsYkH6gMx9y3mRkmcJ2mFUjTd0qNQQvY9VYZgQd7DcN7env35GzlmFaZ23JGp" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
            // customised options
            // • auto-render specific keys, e.g.:
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false},
                {left: '\\(', right: '\\)', display: false},
                {left: '\\[', right: '\\]', display: true}
            ],
            // • rendering keys, e.g.:
            throwOnError : false
            });
        });
    </script>
    <!--END-->
</head>
<body>
    <nav class="navbar sticky-top">
        <div class="container-fluid">
            <div class="navbar-header">
                <!-- Edit here -->
                <a class="navbar-brand" href="../index.html">My WebPages</a>
            </div>
            <botton>
                <span class="nav-hamburger-menu-icon hide"><img src="../assets/menu_hamburger.png" alt="menu"></span>
            </botton>
            <div class="collapse navbar-collapse">
                <ul>
                    <!--Section links. Edit Here -->
                    <li><a href="#intro">Intro</a></li>    
                    <li>
                        <a href="#cg">Conjugate Gradient</a>
                        <ul>
                            <li>
                                <a href="#claim-1">claim 1</a>
                            </li>
                            <li>
                                <a href="#claim-2">claim 2</a>
                            </li>
                            <li>
                                <a href="#claim-3">claim 3</a>
                            </li>
                            <li><a href="#gs-conjugation">Gram Schimdt Conjugation</a></li>
                            <li><a href="#claim-4">cliam 4</a></li>
                            <li><a href="#claim-5">claim 5</a></li>
                            <li><a href="#claim-6">claim 6</a></li>
                        </ul>
                    </li>
                    <li><a href="#cg-algo">CG Algorithm Implementations</a></li>
                </ul>
            </div>
        </div>
    </nav>
    <div id="side-bar">
        <ul>
            <!--Section links. Edit Here-->
            <li><a href="#intro">Intro</a></li>
            <li>
                <a href="#cg">Conjugate Gradient</a>
                <ul>
                    <li>
                        <a href="#claim-1">claim 1</a>
                    </li>
                    <li>
                        <a href="#claim-2">claim 2</a>
                    </li>
                    <li>
                        <a href="#claim-3">claim 3</a>
                    </li>
                    <li><a href="#gs-conjugation">Gram Schimdt Conjugation</a></li>
                    <li><a href="#claim-4">cliam 4</a></li>
                    <li><a href="#claim-5">claim 5</a></li>
                    <li><a href="#claim-6">claim 6</a></li>
                </ul>
            </li>
            <li><a href="#cg-algo">CG Algorithm Implementations</a></li>
        </ul>
    </div>
    <div id="main-content">
        <section id="intro">
            <h2>Intro</h2>
            <p>
                We discuss the conjugate gradient algorithms in details and then we move on and show its application in deblurring images that is blurred by box blur kernel with odd sizes. 
            </p>
            <br>
            <p>
                Here is a list of things that are going to be discussed on this webpage: 
            </p>
            <ol>
                <li>
                    Conjugate Gradient Algorithm
                    <ul>
                        <li>
                            Conjugate Direction and Conjugation Process
                        </li>
                        <li>
                            <b>Proof</b>
                        </li>
                        <li>
                            The algorithmic implementation
                        </li>
                        <li>
                            Augmentations and adaptations
                        </li>
                    </ul>
                </li>
                <li>
                    Box Blur
                    <ul>
                        <li>
                            Kernel convolution on image is matrix vector multiplications
                        </li>
                        <li>
                            Box Blur's matrix representation is a symmetric matrix 
                        </li>
                    </ul>
                </li>
                <li>
                    Applying the conjugate gradient algorithm for deblurring imaged blurred by box blur
                </li>
            </ol>
            <br>
            <p>Here is what is needed from the readers: </p>
            <ul>
                <li>
                    Knows non-trivial amount of multi-variable calculus and linear algebra. 
                </li>
                <li>
                    Knows non-trivial amount of python programming.
                </li>
                <li>
                    Has a background on Scientific Computing. 
                </li>
            </ul>
            Basically if you are a traditional type of engineer, you know are good to go. 
            <br>
            <p>Here is what recommended to know in advanced so that reading can be sped up for the readers</p>
            <ul>
                <li>Properties of the Hermitian Matrices</li>
                <li>The Gram Schmidt Process</li>
                <li>Image Processing</li>
                <li>Positive Definite Matrix</li>
                <li>Graphs and Adjacency Matrix</li>
                <li>Basics about iteratives solvers for linear systems</li>
                <li>Gradient Descned: Steeptest Descend</li>
            </ul>
            <h3>Why all the "Prerequisite"?</h3>
            <blockquote>
                It's not that I want to limit the readers for this webpage, it's just that if I were to explain everything from the ground up, this website will be 200 pages long. 
            </blockquote>
            <h3>Why are all these math necessary?</h3>
            <blockquote>
                A lot of programmer won't care about this, but they will be making a big mistake by not knowing it from a deeper level, it will inhibit their creativity when it comes to problem solving, limit their ability to find noval applications for the algorithms and then when the algorithms failed to do what they expected, blaming the algorithm instead of knowing exactly why and suggest alternatives. 
                <br>
                Personally, I love the math and it's the math that make the Conjugate Gradient Algorithm special. It has a very interesting flavor to it. 
            </blockquote>
            <h3>Jupyter Notebook</h3>
            <p>
                Some of the experimentation with my homebrewed conjugate gradient can be found <a href="./Reverse_Boxblur_CG.html">here </a> in a jupyter notebook. 
            </p>
        </section>
        <hr>
        <section id="cg">
            <h2>Conjugate Gradient Method</h2>
            <p>
                Conjugate gradient is a class of optimizers for linear system. Some of other solvers of the same types includes: Bi-Conjugate Gradient, Bi-Conjugate Gradient Stabalized, and Generalized Minimial Residual Method, GMRes. 
            </p>
            <p>
                These class of algorithm is iterative, meaning that it approximates the solution to the linear system $Ax = b$ better and better. They also comes with the advantage of not requiring to know what numbers are in the matrix $A$, unlike direct method such as Gaussian Eliminations, Cholesky or LU Decomposition. It can figure out the solution purey by interacting with the linear operator $A$. 
            </p>
            <p>
                Their major applications are for solving PDEs, where differential operators such as the Laplacian are used and they needs to be inverted. 
            </p>
            <h3 id="claim-1">Claim 1</h3>
            <blockquote>
                The solution to: 
                $$
                \arg\min_{x}\left\lbrace \frac{1}{2}x^TAx - b^T x + c \right\rbrace
                $$
                Is the solution to $Ax = b$  if $A \in \mathbb{R}^{n\times n}$ is Symmetric Positive Definite (SPD)
            </blockquote>
            <h3>Proof</h3>
            <p>
                The problem has a minima becase $A$ is positive definite, meaning that $x^TAx$ is always going to be larger than zero, therefore it's bounded from below and has a minima. 
            </p>
            <p>The minima can be computed via: </p>
            <p>
                $$
                \begin{aligned}
                f(x) &=\frac{1}{2}x^TAx - b^T x + c
                \\
                \nabla_x[f(x)] &= \frac{1}{2}(A^T + A)x - b
                \\
                \underset{[1]}{\implies}
                \nabla_x[f(x)] &= Ax - b
                \\
                \text{Let: }\quad \nabla_x[f(x)] &= 0
                \\
                \implies 
                    Ax & = b  
                \end{aligned}
                \tag{1}
                $$
            </p>
            <p>[1]: $A + A^T$ is $2A$ because $A$ is symmetric</p>
            <p>Claim 1 is proven $\blacksquare$</p>
            <h3>Inspirations for Conjugate Gradient</h3>
            <p>
                Steepest descend method doesn't work well when the matrix is not well conditioned. It will oscilates along the direction and slowly zig zag to the minimal, turning exactly 90 degress each time. We want a smart way of choosing the direction to descend to such that the energy normal always decrease. (The energy norm: $\Vert x\Vert_A^2 = x^TAx$) 
            </p> 
            <h3>Conjugate Vectors</h3>
            <blockquote>
                Vector $u, v$ are conjugate if $u^TAv = 0$ for some $A$ that is PD (Positive Definite) 
            </blockquote>
            <br>
            <p>Let $x^{(i)}$ be the solution approximated at $i$ th step of the algorithm, let $d^{(i)}$ be the direction of descend chosen at $i$ th iterations of the algorithm, then, we want the algorithm to statisfies the following: </p>
            <p>
                $$
                x^{(k + 1)} = x^{(k)} + \alpha_{k} d^{(k)}
                \tag{2}
                $$
                $$
                d^{(k + 1)} \perp \left\langle 
                    d^{(1)}, d^{(2)}, \cdots,  d^{(k)}
                \right\rangle_A
                \tag{3}
                $$
            </p>
            <p>
                Where, the direction from the $k + 1$ th iteration is conjugate to all previous search direction and $\alpha$ is a step size that is left to be determined. 
            </p>
            <p>
                Before we start, let's define a list of quantities that we are going to use: 
                <h4>Defined Quantities</h4>
                <ol>
                    <li>$Ax_+ = b$</li>
                    <li>$e^{(i)} = x^{(i)} - x_+$</li>
                    <li>$r^{(i)} = b - Ax^{(i)}$</li>
                    <li>$r^{(i)} = -Ae^{(i)} = -A(x^{(i)} - x_+) = -Ax^{(i)} + b$</li>
                    <li>$r^{(i)} = -\nabla_x[f](x^{(i)})$</li>
                </ol>
            </p>
            <h3 id="claim-2">Claim 2</h3>
            <blockquote>
                By representing the error vector as a linear combinations of the conjugate vectors, we an show that the algorithm termintes after at most n steps of literations, if we walk along the direction $d^{(k)}$ such that it minimizes the objective function $f(x)$. 
                $$
                \begin{aligned}
                    e^{(0)} &= \sum_{j = 0}^{n - 1}\delta_jd^{(j)} 
                \end{aligned}
                \tag{4} \implies e^{(n)} = \mathbf{0}
                $$
            </blockquote>
            <h3 id="claim-3">Claim 3</h3>
            <blockquote>
                The step size into the conjugate direction: $\alpha_k$ is the same as the weight given to representing the error vector as linear combinations of the conjugate directions, and it also gives the steepest descend along the conjugate direction for the objective function. 
                $$
                    \delta_k = -\alpha_k
                    \tag{5}
                $$
            </blockquote>
            <h3>Consider Claim 2</h3>
            <p>
                $$
                \begin{aligned}
                    e^{(0)} &= \sum_{j = 0}^{n - 1}\delta_jd^{(j)} 
                    \\
                    Ae^{(0)} &= 
                    \sum_{j = 0}^{n - 1}
                        \delta_j Ad^{(j)}
                    \\
                    d^{(k)}Ae^{(0)} 
                    &=
                    \underbrace{\sum_{j = 0}^{n - 1}
                        \delta_j d^{(k)T}Ad^{(j)}}_{\delta_kd^{(k)}Ad^{(k)}}
                    \\
                    \delta_k 
                    &= 
                    \frac{d^{(k)T}Ae^{(0)}}
                    {
                        d^{(k)T}Ad^{(k)}
                    }
                    \\
                    \underset{
                        [1]
                    }
                    {\implies}
                    \delta_k
                    &= 
                    \frac{
                        d^{(k)T}A(e^{(0)} + \sum_{j = 0}^{k-1} \alpha_jd^{(j)})
                    }
                    {
                        d^{(k)T}Ad^{(k)}
                    }
                    \\
                    \underset{[2]}{
                        \implies
                    }
                    \delta_k &= 
                    \frac{
                        d^{(k)T}Ae^{(k)}
                    }{
                        d^{(k)T}Ad^{(k)}
                    }
                    \\
                    \delta_k &= 
                    \frac{-d^{(k)T}r^{(k)}}{\Vert d^{(k)}\Vert_A^2}
                \end{aligned}
                \tag{6}
                $$
            </p>
            <p>
                [1]: At this point, we just added $\sum_{j = 0}^{k - 1}\alpha_jd^{j}$, but because all the $d^{(i)} \;\forall\; 0 \le i \le k - 1$ are $A$ orthogonal to $d^{(k)}$, so expanding it out will just give us zero. 
                <br>
                [2]: Because $e^{(k)} = x^{(k)} - x^{+} = x^{(0)} + \sum_{j = 0}^{k - 1}\alpha_kd^{(k)} - x^{+} = e^{(0)} + \sum_{j = 0}^{k - 1}\alpha_kd^{(k)}$. Recalled section <b>Defined Quantities. </b>
            </p>
            <h3>Claim 3 Proof</h3>
            <p>
                Optimizing the function along the conjugate direction requires considering the optimality conditions forest, by taking the derivative wrt $\alpha_k$ and then set it to zero: 
            </p>
            <p>
                $$
                \begin{aligned}
                    \partial_\alpha [f(x^{(k + 1)})] &= 
                    \partial_\alpha[f(x^{(k)} + \alpha d^{(k)})]
                    \\
                    &= 
                    \nabla_x[f(x^{(k + 1)})]^Td^{(k)}
                    \\
                    \text{set:}\quad 
                    \partial_\alpha [f(x^{(k + 1)})] &= 0
                    \\
                    \underset{[1]}{\implies}
                    r^{(k + 1)T}d^{(k)} &= 0
                    \\
                    \implies 
                    Ae^{(k + 1)T}d^{(k)} &= 0
                \end{aligned}
                \tag{7}
                $$
            </p>
            <p>
                [1]: Recall that $r^{(k)} = b - Ax^{(k)}$ and $\nabla_x[f(x)] = Ax - b$ from Defined Quantities. 
            </p>
            <p>
                However, the optimality conditions doesn't seem to involve the qautntity $\alpha$ explictly, therefore we need to find it by expanding on $e^{(k + 1)}$, so then we have: 
            </p>
            <p>
                $$
                \begin{aligned}
                    r^{(k + 1)T}d^{(k)} 
                    &= 0
                    \\
                    [b - Ax^{(k + 1)}]^Td^{(k)} &= 0
                    \\
                    [b - A(x^{(k)} + \alpha_k  d^{(k)})]^Td^{(k)} &= 0
                    \\
                    [b - Ax^{(k)} - \alpha_k A d^{(k)}]^Td^{(k)} &= 0
                    \\
                    [r^{(k)} - \alpha_k Ad^{(k)}]^Td^{(k)} &= 0 
                    \\
                    \alpha_k &= \frac{r^{(k)T}d^{(k)}}{d^{(k)T}Ad^{(k)}}
                    \\
                    \alpha_k &= \frac{r^{(k)T}d^{(k)}}
                    {
                        \Vert d^{(k)}\Vert_A^2
                    }
                \end{aligned}\tag{8}
                $$
            </p>
            <p>Compare $\alpha_k$ with $\delta_k$ from (8) and (7) and observe that claim 3 has been proven. $\blacksquare$</p>
            <p>Using the fact from claim 3, and then the definition of $e^{(0)}$, observe that if conjugate gradient is tshe search direction and we use the $\alpha_k$ found in the proof, then claim 2 is proved as well. $\blacksquare$</p>
            <h3>Morals of the story so far</h3>
            <p>
                As we can see, there is indeed a way of choosing the directions of descend such that the convergence after $n$ interations is promised, and we can also observe that, the error vector, when measured using energy norm, is always decreasing too. $\Vert e^{(k + 1)}\Vert \le \Vert e^{(k)}\Vert_A$. 
            </p>
            <h3>Intuitions</h3>
            <p>
                As explained in different literatures, there are a lot of different interpretations in choosing conjugate direction and how it exactly decreases the objective function in such an optimal way. 
            </p>
            <p>
                Most literatures explain it that, the algorithm will choose the search direction such that the energy norm with matrix $A$ is decreased as much as possible. 
            </p>
            <p>
                Some other suggests the geometric interpretation of the fact that conjugate vectors are orthogonal after being transformed with matrix $A^{-1}$, and it's up to the reader to know that $A^{-1}$ exists when $A$ is symmetric and positive definite. 
            </p>
            <p>
                Well, my interpretations is based on eigenvectors. If you make $d^{(0)}$ to be the eigenvector of $A$, then the algorithm will find the solution right away because the eigenvector of the Hermitian Matrix has the geometric interpretations that they are the principal axis of the transformation. (Or some algebra can also be a convincing argument)
                <br>
                In addition, using the fact that eigenvectors of matrix $A$ are orthogonal, then eigenvectors they are conjugate of each other. And, if the vector $u, v$ when represented under the eigenspace are orthogonal, it's not hard to see that they will also be orthogonal. Now recall that Eigenvectors are also the principal axises for the transformations of the Hermitian matrix. Think about it in your head and you will know why everything makes sense.  That is my interpretation. 
            </p>
            <h3>There is still a Big Problem</h3>
            <blockquote>
                How do we find Conjugate Vectors as search direction for the algorithm?     
            </blockquote>
        </section>
        <hr>
        <section id="gs-conjugation">
            <h2 id="gs-conjugation">Gram Schimdt Conjutation</h2>
            <p>
                It's the same idea as the Gram Schimdt orthogonalization process, but this time we wish to produce conjugate vectors instead of orthogonal vectors. 
                <ul>
                    <li>GS Conjugation: Orthogonal Vectors $\rightarrow$ Conjugate Vectors</li>
                    <li>GS Orthogonalization: Any Linearly Independent vectors $\rightarrow$ Orthogonal Vectors</li>
                </ul>
            </p>
            <blockquote>
                Given a set of orthogonal vectors $\{u_i\}_{i = 1}^n$ that spans the whole $\mathbb{R}^{n}$ (Standard Basis vectors are an ok choice here), where, the matrix $A$ is $n\times n$.
                <br>
                To construct a set of vectors that are $A$ orthogonal, we would need to subract from the vector $u_i$ with components span by the vectors $d^{(i< k)}$. Mathematically:
                $$
                \begin{aligned}
                    d^{(k)} &= u_{k} + \sum_{i = 1}^{k - 1}
                        \beta_{k, i} d^{(i)}
                \end{aligned}
                \tag{9}
                $$
            </blockquote>
            <h3>Looking for $\beta_{k, i}$</h3>
            Choose $m < k$ and consider: 
            $$
                \begin{aligned}
                    d^{(m)T}Ad^{(k)} &= d^{(m)T}Au_{k} + d^{(m)T}A 
                        \sum_{i = 1}^{k - 1}\beta_{k, i}d^{(i)}
                    \\
                    0 &= d^{(m)T}Au_{k} + \beta_{k, m}d^{(m)T}Ad^{(m)}
                    \\
                    \beta_{k, m} &= 
                    - \frac
                    {
                        d^{(m)T}Au_{k}
                    }
                    {
                        \Vert d^{(m)}\Vert_A^2
                    }
                    \\
                    \implies 
                    d^{(k)} &= u_{k} - \sum_{i = 1}^{k - 1}
                    \frac{
                        d^{(i)T}Au_{k}
                    }{
                        \Vert d^{(i)}\Vert_A^2
                    }
                    d^{(i)}
                \end{aligned}
                \tag{10}
            $$
            <h4>Exercise for the Readers</h4>
            <blockquote>
                Reformulate the GS Conjugation without the orthogonality assumption on $u_k$. I will be too easy on the readers if it's just me who is doing the math. Regardless of who you are, I encourage you to think about it, because you are reading it, you should think about it even if you are a recruiter who stumbled upon my websites.
            </blockquote>
            
            <h3>The Magical Ingredient: $r^{(k)}$</h3>
            <p>
                Using the residual vector the guide the generative process of the conjugate directions will tremendously reduce the complexity of the GS conjugation, because it won't need all previous $d^{(i < k - 1)}$ vectors anymore. 
            </p>
            <h4 id="claim-4">Claim 4</h4>
            <blockquote>
                The residual direction at the $j$ of the iteration is orthogonal to all previous conjugate direction, assuming conjugate directions for the steepest gradient descend, mathematically: 
                $$r^{(j)T}d^{(i)} = 0 \quad \forall i < j$$
            </blockquote>
            <h4>Claim 4 Proof</h4>
            <p>Recalled from Defined Quantities, let $i < j$ and Consider</p>
            $$
            \begin{aligned}
                e^{(k)} &= e^{(0)} + \sum_{j = 0}^{k - 1}\alpha_jd^{(j)}
                \\
                \underset{(4)}{\implies}
                e^{(k)}&= 
                e^{(0)} - \sum_{j = 0}^{k - 1} \delta_jd^{(j)}
                \\
                \implies
                e^{(k)}&= 
                \sum_{j = k}^{n - 1} \delta_j d^{(j)}
                \\
                \underset{\text{let: } k \leftrightarrow j}{\implies}
                e^{(j)} &= \sum_{k = j}^{n - 1} \delta_kd^{(k)}
                \\
                Ae^{(j)} &= \sum_{k = j}^{n - 1}
                    \delta_k Ad^{(k)}
                \\
                -d^{(i)T}Ae^{(j)} &= \underbrace{-d^{(i)T}\sum_{k = j}^{n - 1}\delta_k Ad^{(k)}}_{= 0}
                \\
                d^{(i)}r^{(j)} &= 0 \quad \forall\; i < j
            \end{aligned}\tag{11}
            $$
            <p>Claim 4 is proven $\blacksquare$</p>
            <h4 id="claim-5">Claim 5</h4>
            <blockquote>
                In addition to $r^{(k)}$ being orthogonal to all previous conjugate vectors, it's also authogonal to all previous $u_i$ that assists with the generative process of the conjugate vectors.
                $$r^{(i)}\perp u_{j} \quad \forall i < j$$
            </blockquote>
            <h4>Claim 5 Proof</h4>
            <p>Choose $j >  i$ then consider:</p>
            $$
            \begin{aligned}
                d^{(i)} &= u_i + \sum_{k = 0}^{i - 1} \beta_{i, k}d^{(k)}
                \\
                r^{(j)T}d^{(i)} &= r^{(j)T}u_i + r^{(j)T}\sum_{k = 0}^{i - 1}\beta_{i, k}d^{(k)}
                \\
                \underset{\text{claim 4}}{\implies} 0 &= r^{(j)T}u_i
            \end{aligned}\tag{12}
            $$
            <p>Observe that, choosing $i = j$, we have $r^{(i)T}d^{(i)} = r^{(i)T}u_i$</p>
            <p>Claim 5 is proven $\blacksquare$</p>
            <h4 id="claim-6">Claim 6</h4>
            <p>
                We are getting close to the essence of the conjugate gradient algorithm. 
            </p>
            <blockquote>
                Using the residual vector during the iterations will simplify the GS Conjugation process significantly. basically let $u_i = r^{(i)}$. 
            </blockquote>
            <br>
            <h4>Claim 6 Justification</h4>
            <p>Congrat you made all the way to here, the next few steps is basically using the residual vectors during the iterations as the set of $\{u_i\}_{i = 1}^n$ for assisting the generation of the conjugate search directions. </p>
            <p>
                From the Gram Schimdt conjugation process we have: 
                $$
                    \beta_{k, m} = 
                    - \frac
                    {
                        d^{(m)T}Au_{k}
                    }
                    {
                        \Vert d^{(m)}\Vert_A^2
                    }
                $$
                Let $u_k = r^{(k)}$  then $\beta_{k, m} = \frac{d^{(m)T}Ar^{(k)}}{\Vert d^{(m)}\Vert_A^2}$ and we need to find $r^{(k)T}Ad^{(m)}$. 
            </p>
            <p>
                Consider the residual at the $j + 1$ iteration of the algorithm then: 
                $$
                \begin{aligned}
                    r^{(j + 1)} &= -Ae^{(j + 1)}
                    \\
                    &= -A(e^{(j)} + \alpha_{j} d^{(j)})
                    \\
                    &= r^{(j)} - \alpha_{j} Ad^{(j)}
                    \\
                    r^{(i)T}r^{(j + 1)} &=  r^{(i)T}r^{(j)} - \alpha_{j} r^{(i)T}Ad^{(j)}
                    \\
                    \alpha_{j}r^{(i)T}Ad^{(j)} &= r^{(i)T}r^{(j)} - r^{(i)}r^{(j + 1)}
                \end{aligned}
                \tag{14}
                $$
            </p>
            <p>
                Now, there are several cases of values for $\alpha_i$, and it depends on the value of $i, j$. 
                <ul>
                    <li>Consider $i < j$, from claim 5, replacing $u_j$ with $r^{(j)}$, when $i < j$, the right handside is always zero, which implies that $\beta_{i, j} = 0$. </li>
                    <li>When $i = j$ it's $\Vert r^{(i)}\Vert_2^2$, however we don't need this for the conjutation, it's not in the summation. </li>
                    <li>Consider $i = j + 1$, then </li>
                </ul>
                $$
                \begin{aligned}
                \alpha_{j}r^{(j + 1)T}Ad^{(j)} &= r^{(j + 1)T}r^{(j)} - r^{(j + 1)}r^{(j + 1)}
                \\
                r^{(j + 1)T}Ad^{(j)} &= \frac{-\Vert r^{(j + 1)}\Vert_2^2}{\alpha_j}
                \\
                \frac{r^{(j + 1)T}Ad^{(j)}}{\Vert d^{(j)}\Vert_A^2}
                &= 
                \frac{-\Vert r^{(j + 1)}\Vert_2^2}{\alpha_j \Vert d^{(j)}\Vert_A^2}
                \\
                -\beta_{j + 1, j} &= \frac{-\Vert r^{(j + 1)}\Vert_2^2}{\alpha_j \Vert d^{(j)}\Vert_A^2}
                \\
                \beta_{j + 1, j} &= \frac{\Vert r^{(j + 1)}\Vert_2^2}{\alpha_j \Vert d^{(j)}\Vert_A^2}
                \end{aligned}\tag{15}
                $$
            </p>
            <p>
                The only quantity left is when $i = j + 1$, and therefore, we only need to keep the residual vector from the preious iterations and the conjugate direction to figure out the next conjugate search direction. At this point, claim 6 has been justified. $\blacksquare$
            </p>
        </section>
        <hr>
        <section id="cg-algo">
            <h2>The Conjugate Gradient Algorithm</h2>
            <p>
                Now, we have all the components for the Conjugate Gradient Algorithm. Let's take a look at the quantities that need to be updated and maintained throughout the algorithm
                <ul>
                    <li>
                        $\alpha_k$, the step size into the conjugate direction. 
                    </li>
                    <li>
                        $\beta_{k + 1, k}$, the weighting for the next conjugate direction. 
                    </li>
                    <li>
                        $r^{(k)}$, $d^{(k)}$, $x^{(k)}$, the usual deal.
                    </li>
                </ul>
            </p>
            <p>Let's consider $\beta_{i + 1, i}$, and recall that from claim 3, we have $\alpha_k = \frac{r^{(k)T}d^{(k)}}{\Vert d^{(k)}\Vert_A^2}$</p>
            <p>
                Recall from the proof of claim 5, with the observation $r^{(i)T}d^{(i)} = r^{(i)}u_i$, and we know that $u_i = r^{(i)}$, then we have: 
                $$
                    \begin{aligned}
                        \alpha_k &= 
                        \frac{r^{(k)T}r^{(k)}}{\Vert d^{(k)}\Vert_A^2}                    
                        \\
                        \implies 
                        \beta_{j + 1, j} &= \frac{\Vert r^{(j + 1)}\Vert_2^2}{\Vert r^{(j)} \Vert_2^2}
                    \end{aligned}
                $$
            </p>
            <p>
                And here is the final updating sequence for each of the variable, for notation convenience, $\beta_{i + 1, i}$ is $\beta_{i + 1}$, and then we have: 
                $$
                    \begin{aligned}
                        d^{(0)} &= b - Ax^{(0)} 
                        \\
                        \alpha_{i} &= \frac{\Vert r^{(i)}\Vert_2^2}
                        {\Vert d^{(i)}\Vert_A^2}
                        \\
                        x^{(i + 1)} &= x^{(i)} + \alpha_i d^{(i)}
                        \\
                        r^{(i + 1)} &= r^{(i)} - \alpha_iAd^{(i)} = b - Ax^{(i + 1)}
                        \\
                        \beta_{i + 1} &= \frac{\Vert r^{(j + 1)}\Vert_2^2}{\Vert r^{(i)}\Vert_2^2}
                        \\
                        d^{(i + 1)} &= r^{(i + 1)} + \beta_{i + 1}d^{(i)}
                    \end{aligned}
                $$
            </p>
            <h3>CG implemented Python</h3>
            <pre>
                <code class="python hljs">
def AbstractCGGenerate(A:callable, b, x0, maxitr:int=100):
    """
        A generator for the Conjugate Gradient method, so whoever uses it
        has the pleasure to collect all the quantities at runtime.
    :param A:
    :param b:
    :param x0:
    :return:
        itr, r, x
    """
    x = x0
    d = r = b - A(x)
    Itr = 0
    while Itr < maxitr:
        alpha = np.sum(r * r) / np.sum(d * A(d))
        if alpha < 0:
            warnings.warn(f"CG negative energy norm! Matrix Not symmetric and SPD! Convergence might fail.")
        x += alpha * d
        # rnew = r - alpha * A(d)
        rnew = b - A(x)
        beta = np.sum(rnew * rnew) / np.sum(r * r)
        d = rnew + beta * d
        r = rnew
        Itr += 1
        yield Itr, r, x
                </code>
            </pre>
            <h4>A list of Things for the Algorithm</h4>
            <ul>
                <li>It's a generic subroutine.</li>
                <li>For better numerical precision, we implemented $b - Ax^{(i + 1)}$ for getting the residual</li>
                <li>The algorithm almost never converge complete after $n$ iterations because of numerical instability</li>
                <li>This algorithm treat $A$ as a blackbox, and all vectors can be left as tensor, representing images, or whatever it's under the concern for the linear operator, but it has to be a numpy arrray type</li>
                <li>The positive definite precondition of the algorithm puts a warning for the user instead of asserting it. </li>
                <li>This algorithm doesn't support the use of preconditioners because I haven't talk about thta part yet. </li>
            </ul>
            <h3>Exercise for the Readers</h3>
            <blockquote>
                I don't care who you are, but doing this will enhance your understanding of what we just discussed. 
                <ol>
                    <li>Implement this in C/C++. </li>
                
                    <li>Extra points if it's implemented on GPU. Extra Extra points if it supports preconditioners.</li>
                    
                    <li>Extra Extra Extra points if you got both. </li>
                    
                    <li>Go do your doctoral thesis if you can improve the numerical stability for badly conditioned $A$.(You should not be goofying around on my website and reading this if that is the case).</li>
                </ol>
                
            </blockquote>
            <h3>THE END of Vanilla Conjugate Gradient</h3>
        </section>
        <hr>
        <section id="cg-aug">
            <h2>Adaptations and Augmentaions for Conjugate Gradient</h2>
            <p>
                This section is concerned about applying the algorithm to different situations, and potential interesting behaviors when it's not used properly. Keep in mind that, if an approximation matrix $M$ is found for $A$, then preconditioners can be applied to improve the convergence of the algorithm, but that is skipped here. 
            </p>
            <h3>Symmmetric Positive Semi-Definite Matrices</h3>
            <p>
                Suppose that matrix $A$ is positive semi-definite, meaning that $\exists x: x^TAx = 0$, then according to claim 1, the problem has infinitely many solutions, and it's up to the initial guess to say which one the algorithm will land us on. 
                <br>
                One can find the use of adding some extra terms for the objective function useful, potentially limiting the number of solutions. But such an approach will involve changing the algorithm a bit to support the regularizations. 
            </p>
            <h3>
                Symmetric Matrices
            </h3>
            <p>
                If given matrix that is only symmetric, then it's possible that it's positive semi-definite, negative semi-definite, or definite. In such cases, CG can't be diretly applied. 
                <h4>Assuming that $A$ is Negative Definite, or Semi-Definite</h4>
                <p>
                    Then $-A$ is positive definite or semi-definite. Solve $-Ax = -b$ instead, or make it into gradient ascend instead of descend. 
                </p>
                <h4>Assuming $A$ is Neither</h4>
                <p>Consider</p>
                $$
                    \begin{aligned}
                        Ax &= b
                        \\
                        A^TAx &= A^Tb
                        \\
                        A^2x &= Ab
                    \end{aligned}
                $$
                Then, $A^2$ is positive definite or semi-definite, solve that instead, at the cost of poorer conditioning for the matrix $A^2$. 
            </p>
            <h3>$A$ is not Symmetric</h3>
            <p>
                Then $A^TA$ is symmetric, solve $A^TA = A^Tb$ instead, at the cost of poorer conditioning and there is no way of treating the problem as a blackbox anymore. Or, consider using GMRes algorithm, which is applicable for all types of matrices and it's able to treat then as a blackbox. 
            </p>
        </section>
        <hr>
        <section id="boxblur">
            <h2>Box Blurring</h2>
            <p>
                Let the kernel size be $M\times N$, then the Kernel is given as: 
            </p>
        </section>
        <div class="footer-padding">
        </div>
    </div>
    <footer>
        <!--For footer context botton-->
        <div class="fixed-bottom hide" id="footer-display">
            <div class="row align-items-center" id="footer-display-inner">
                <div class="col text-right">
                    <button type="button" class="btn btn-primary toggle-btn">Toggle Big Printout</button>
                </div>
            </div>
        </div>
        <!--Ends-->
    </footer>
</body>
</html>