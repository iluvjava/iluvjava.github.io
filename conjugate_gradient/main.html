<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">
    <title>L1, L2 Norm as Loss Function</title>

    <!--For Bootstrap style-->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
        integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!--Bootstrap end-->

    <!--MyStyle-->
    <link rel="stylesheet" href="mystyle.css">
    <!--End-->

    <!--For plotting data and Adpating Webpage to mobile devices-->
    <script src="client_scripts_general.js"></script>
    <!-- End -->

    <!--Plotly-->
    <script src="https://cdn.plot.ly/plotly-1.2.0.min.js"></script>
    <!--FIX VERSION, new UPDATE has problem-->
    <!--END-->

    <!--JS hilight for the code blocks in the web: -->
    <link rel="stylesheet" href="../assets/client_scripts/syntax_highlight_pack/styles/a11y-light.css">
    <script src="../assets/client_scripts/syntax_highlight_pack/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <!--END-->
    <!--For Katex -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js" integrity="sha384-YNHdsYkH6gMx9y3mRkmcJ2mFUjTd0qNQQvY9VYZgQd7DcN7env35GzlmFaZ23JGp" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
            // customised options
            // • auto-render specific keys, e.g.:
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false},
                {left: '\\(', right: '\\)', display: false},
                {left: '\\[', right: '\\]', display: true}
            ],
            // • rendering keys, e.g.:
            throwOnError : false
            });
        });
    </script>
    <!--END-->
</head>
<body>
    <nav class="navbar sticky-top">
        <div class="container-fluid">
            <div class="navbar-header">
                <!-- Edit here -->
                <a class="navbar-brand" href="../index.html">My WebPages</a>
            </div>
            <botton>
                <span class="nav-hamburger-menu-icon hide"><img src="../assets/menu_hamburger.png" alt="menu"></span>
            </botton>
            <div class="collapse navbar-collapse">
                <ul>
                    <!--Section links. Edit Here -->
                    <li><a href="#intro">Intro</a></li>    
                    
                </ul>
            </div>
        </div>
    </nav>
    <div id="side-bar">
        <ul>
            <!--Section links. Edit Here-->
            <li><a href="#intro">Intro</a></li>
            
        </ul>
    </div>
    <div id="main-content">
        <section id="intro">
            <h2>Intro</h2>
            <p>
                We discuss the conjugate gradient algorithms in details and then we move on and show its application in deblurring images that is blurred by box blur kernel with odd sizes. 
            </p>
            <br>
            <p>
                Here is a list of things that are going to be discussed on this webpage: 
            </p>
            <ol>
                <li>
                    Conjugate Gradient Algorithm
                    <ul>
                        <li>
                            Conjugate Direction and Conjugation Process
                        </li>
                        <li>
                            <b>Proof</b>
                        </li>
                        <li>
                            The algorithmic implementation
                        </li>
                        <li>
                            Augmentations and adaptations
                        </li>
                    </ul>
                </li>
                <li>
                    Box Blur
                    <ul>
                        <li>
                            Kernel convolution on image is matrix vector multiplications
                        </li>
                        <li>
                            Box Blur's matrix representation is a symmetric matrix 
                        </li>
                    </ul>
                </li>
                <li>
                    Applying the conjugate gradient algorithm for deblurring imaged blurred by box blur
                </li>
            </ol>
            <br>
            <p>Here is what is needed from the readers: </p>
            <ul>
                <li>
                    Knows non-trivial amount of multi-variable calculus and linear algebra. 
                </li>
                <li>
                    Knows non-trivial amount of python programming.
                </li>
                <li>
                    Has a background on Scientific Computing. 
                </li>
            </ul>
            Basically if you are a traditional type of engineer, you know are good to go. 
            <br>
            <p>Here is what recommended to know in advanced so that reading can be sped up for the readers</p>
            <ul>
                <li>Properties of the Hermitian Matrices</li>
                <li>The Gram Schmidt Process</li>
                <li>Image Processing</li>
                <li>Positive Definite Matrix</li>
                <li>Graphs and Adjacency Matrix</li>
                <li>Basics about iteratives solvers for linear systems</li>
            </ul>
            <h3>Why all the "Prerequisite"?</h3>
            <blockquote>
                It's not that I want to limit the readers for this webpage, it's just that if I were to explain everything from the ground up, this website will be 200 pages long. 
            </blockquote>
            <h3>Why are all these math necessary?</h3>
            <blockquote>
                A lot of programmer won't care about this, but they will be making a big mistake by not knowing it from a deeper level, it will inhibit their creativity when it comes to problem solving, limit their ability to find noval applications for the algorithms and then when the algorithms failed to do what they expected, blaming the algorithm instead of knowing exactly why and suggest alternatives. 
                <br>
                Personally, I love the math and it's the math that make the Conjugate Gradient Algorithm special. It has a very interesting flavor to it. 
            </blockquote>
        </section>
        <hr>
        <section id="cg">
            <h2>Conjugate Gradient Method</h2>
            <p>
                Conjugate gradient is a class of optimizers for linear system. Some of other solvers of the same types includes: Bi-Conjugate Gradient, Bi-Conjugate Gradient Stabalized, and Generalized Minimial Residual Method, GMRes. 
            </p>
            <p>
                These class of algorithm is iterative, meaning that it approximates the solution to the linear system $Ax = b$ better and better. They also comes with the advantage of not requiring to know what numbers are in the matrix $A$, unlike direct method such as Gaussian Eliminations, Cholesky or LU Decomposition. It can figure out the solution purey by interacting with the linear operator $A$. 
            </p>
            <p>
                Their major applications are for solving PDEs, where differential operators such as the Laplacian are used and they needs to be inverted. 
            </p>
            <h3>Claim 1</h3>
            <blockquote>
                The solution to: 
                $$
                \arg\min_{x}\left\lbrace \frac{1}{2}x^TAx - b^T x + c \right\rbrace
                $$
                Is the solution to $Ax = b$  if $A \in \mathbb{R}^{n\times n}$ is Symmetric Positive Definite (SPD)
            </blockquote>
            <h3>Proof</h3>
            <p>
                The problem has a minima becase $A$ is positive definite, meaning that $x^TAx$ is always going to be larger than zero, therefore it's bounded from below and has a minima. 
            </p>
            <p>The minima can be computed via: </p>
            <p>
                $$
                \begin{aligned}
                f(x) &=\frac{1}{2}x^TAx - b^T x + c
                \\
                \nabla_x[f(x)] &= \frac{1}{2}(A^T + A)x - b
                \\
                \underset{(1)}{\implies}
                \nabla_x[f(x)] &= Ax - b
                \\
                \text{Let: }\quad \nabla_x[f(x)] &= 0
                \\
                \implies 
                    Ax & = b  
                \end{aligned}
                \tag{1}
                $$
            </p>
            <p>(1): $A + A^T$ is $2A$ because $A$ is symmetric</p>
            <p>Claim 1 is proven $\blacksquare$</p>
            <h3>Inspirations for Conjugate Gradient</h3>
            <p>
                Steepest descend method doesn't work well when the matrix is not well conditioned. It will oscilates along the direction and slowly zig zag to the minimal, turning exactly 90 degress each time. We want a smart way of choosing the direction to descend to such that the energy normal always decrease. (The energy norm: $\Vert x\Vert_A^2 = x^TAx$) 
            </p> 
            <h3>Conjugate Vectors</h3>
            <blockquote>
                Vector $u, v$ are conjugate if $u^TAv = 0$ for some $A$ that is PD (Positive Definite) 
            </blockquote>
            <br>
            <p>Let $x^{(i)}$ be the solution approximated at $i$ th step of the algorithm, let $d^{(i)}$ be the direction of descend chosen at $i$ th iterations of the algorithm, then, we want the algorithm to statisfies the following: </p>
            <p>
                $$
                x^{(k + 1)} = x^{(k)} + \alpha_{k} d^{(k)}
                \tag{2}
                $$
                $$
                d^{(k + 1)} \perp \left\langle 
                    d^{(1)}, d^{(2)}, \cdots,  d^{(k)}
                \right\rangle_A
                \tag{3}
                $$
            </p>
            <p>
                Where, the direction from the $k + 1$ th iteration is conjugate to all previous search direction and $\alpha$ is a step size that is left to be determined. 
            </p>
            <p>
                Before we start, let's define a list of quantities that we are going to use: 
                <h4>Defined Quantities</h4>
                <ol>
                    <li>$Ax_+ = b$</li>
                    <li>$e^{(i)} = x^{(i)} - x_+$</li>
                    <li>$r^{(i)} = b - Ax^{(i)}$</li>
                    <li>$r^{(i)} = -Ae^{(i)} = -A(x^{(i)} - x_+) = -Ax^{(i)} + b$</li>
                    <li>$r^{(i)} = -\nabla_x[f](x^{(i)})$</li>
                </ol>
            </p>
            <h3>Claim 2</h3>
            <blockquote>
                By representing the error vector as a linear combinations of the conjugate vectors, we an show that the algorithm termintes after at most n steps of literations, if we walk along the direction $d^{(k)}$ such that it minimizes the objective function $f(x)$. 
                $$
                \begin{aligned}
                    e^{(0)} &= \sum_{j = 0}^{n - 1}\delta_jd^{(j)} 
                \end{aligned}
                \tag{4} \implies e^{(n)} = \mathbf{0}
                $$
            </blockquote>
            <h3>Claim 3</h3>
            <blockquote>
                The step size into the conjugate direction: $\alpha_k$ is the same as the weight given to representing the error vector as linear combinations of the conjugate directions, and it also gives the steepest descend along the conjugate direction for the objective function. 
                $$
                    \delta_k = -\alpha_k
                    \tag{5}
                $$
            </blockquote>
            <h3>Consider Claim 2</h3>
            <p>
                $$
                \begin{aligned}
                    e^{(0)} &= \sum_{j = 0}^{n - 1}\delta_jd^{(j)} 
                    \\
                    Ae^{(0)} &= 
                    \sum_{j = 0}^{n - 1}
                        \delta_j Ad^{(j)}
                    \\
                    d^{(k)}Ae^{(0)} 
                    &=
                    \underbrace{\sum_{j = 0}^{n - 1}
                        \delta_j d^{(k)T}Ad^{(j)}}_{\delta_kd^{(k)}Ad^{(k)}}
                    \\
                    \delta_k 
                    &= 
                    \frac{d^{(k)T}Ae^{(0)}}
                    {
                        d^{(k)T}Ad^{(k)}
                    }
                    \\
                    \underset{
                        [1]
                    }
                    {\implies}
                    \delta_k
                    &= 
                    \frac{
                        d^{(k)T}A(e^{(0)} + \sum_{j = 0}^{k-1} \alpha_jd^{(j)})
                    }
                    {
                        d^{(k)T}Ad^{(k)}
                    }
                    \\
                    \underset{[2]}{
                        \implies
                    }
                    \delta_k &= 
                    \frac{
                        d^{(k)T}Ae^{(k)}
                    }{
                        d^{(k)T}Ad^{(k)}
                    }
                    \\
                    \delta_k &= 
                    \frac{-d^{(k)T}r^{(k)}}{\Vert d^{(k)}\Vert_A^2}
                \end{aligned}
                \tag{6}
                $$
            </p>
            <p>
                [1]: At this point, we just added $\sum_{j = 0}^{k - 1}\alpha_jd^{j}$, but because all the $d^{(i)} \;\forall\; 0 \le i \le k - 1$ are $A$ orthogonal to $d^{(k)}$, so expanding it out will just give us zero. 
                <br>
                [2]: Because $e^{(k)} = x^{(k)} - x^{+} = x^{(0)} + \sum_{j = 0}^{k - 1}\alpha_kd^{(k)} - x^{+} = e^{(0)} + \sum_{j = 0}^{k - 1}\alpha_kd^{(k)}$. Recalled section <b>Defined Quantities. </b>
            </p>
            <h3>Claim 3 Proof</h3>
            <p>
                Optimizing the function along the conjugate direction requires considering the optimality conditions forest, by taking the derivative wrt $\alpha_k$ and then set it to zero: 
            </p>
            <p>
                $$
                \begin{aligned}
                    \partial_\alpha [f(x^{(k + 1)})] &= 
                    \partial_\alpha[f(x^{(k)} + \alpha d^{(k)})]
                    \\
                    &= 
                    \nabla_x[f(x^{(k + 1)})]^Td^{(k)}
                    \\
                    \text{set:}\quad 
                    \partial_\alpha [f(x^{(k + 1)})] &= 0
                    \\
                    \underset{[1]}{\implies}
                    r^{(k + 1)T}d^{(k)} &= 0
                    \\
                    \implies 
                    Ae^{(k + 1)T}d^{(k)} &= 0
                \end{aligned}
                \tag{7}
                $$
            </p>
            <p>
                [1]: Recall that $r^{(k)} = b - Ax^{(k)}$ and $\nabla_x[f(x)] = Ax - b$ from Defined Quantities. 
            </p>
            <p>
                However, the optimality conditions doesn't seem to involve the qautntity $\alpha$ explictly, therefore we need to find it by expanding on $e^{(k + 1)}$, so then we have: 
            </p>
            <p>
                $$
                \begin{aligned}
                    r^{(k + 1)T}d^{(k)} 
                    &= 0
                    \\
                    [b - Ax^{(k + 1)}]^Td^{(k)} &= 0
                    \\
                    [b - A(x^{(k)} + \alpha_k  d^{(k)})]^Td^{(k)} &= 0
                    \\
                    [b - Ax^{(k)} - \alpha_k A d^{(k)}]^Td^{(k)} &= 0
                    \\
                    [r^{(k)} - \alpha_k Ad^{(k)}]^Td^{(k)} &= 0 
                    \\
                    \alpha_k &= \frac{r^{(k)T}d^{(k)}}{d^{(k)T}Ad^{(k)}}
                    \\
                    \alpha_k &= \frac{r^{(k)T}d^{(k)}}
                    {
                        \Vert d^{(k)}\Vert_A^2
                    }
                \end{aligned}\tag{8}
                $$
            </p>
            <p>Compare $\alpha_k$ with $\delta_k$ from (8) and (7) and observe that claim 3 has been proven. $\blacksquare$</p>
            <p>Using the fact from claim 3, and then the definition of $e^{(0)}$, observe that if conjugate gradient is tshe search direction and we use the $\alpha_k$ found in the proof, then claim 2 is proved as well. $\blacksquare$</p>
            <h3>Morals of the story so far</h3>
            <p>
                As we can see, there is indeed a way of choosing the directions of descend such that the convergence after $n$ interations is promised, and we can also observe that, the error vector, when measured using energy norm, is always decreasing too. $\Vert e^{(k + 1)}\Vert \le \Vert e^{(k)}\Vert_A$. 
            </p>
            <h3>Intuitions</h3>
            <p>
                As explained in different literatures, there are a lot of different interpretations in choosing conjugate direction and how it exactly decreases the objective function in such an optimal way. 
            </p>
            <p>
                Most literatures explain it that, the algorithm will choose the search direction such that the energy norm with matrix $A$ is decreased as much as possible. 
            </p>
            <p>
                Some other suggests the gemetric interpretation of the fact that conjugate vectors are orthogonal after being transformed with matrix $A^{-1}$, and it's up to the reader to know that $A^{-1}$ exists when $A$ is symmetric and positive definite. 
            </p>
            <p>
                Well, my interpretations is based on eigenvectors. If you make $d^{(0)}$ to be the eigenvector of $A$, then the algorithm will find the solution right away because the eigenvector of the Hermitian Matrix has the geometric interpretations that they are the principal axis of the transformation. 
                <br>
                In addition, using the fact that eigenvectors of matrix $A$ are orthogonal, it's not hard to see eigenvectors they are conjugate of each other. And, if the vector $u, v$ when represented under the eigenspace are orthogonal, it's not hard to see that they will also be orthogonal. That is my interpretation. 
            </p>
            <h3>There is still a Big Problem</h3>
            <blockquote>
                How do we find Conjugate Vectors as search direction for the algorithm?     
            </blockquote>
        </section>
        <hr>
        <section id="gs-conjugation">
            <h2>Gram Schimdt Conjutation</h2>
            <p>
                It's the same idea as the Gram Schimdt orthogonalization process, but this time we wish to produce conjugate vectors instead of orthogonal vectors. 
                <ul>
                    <li>GS Conjugation: Orthogonal Vectors $\rightarrow$ Conjugate Vectors</li>
                    <li>GS Orthogonalization: Any Linearly Independent vectors $\rightarrow$ Orthogonal Vectors</li>
                </ul>
            </p>
            <blockquote>
                Given a set of orthogonal vectors $\{u_i\}_{i = 1}^n$ that spans the whole $\mathbb{R}^{n}$ (Standard Basis vectors are an ok choice here), where, the matrix $A$ is $n\times n$.
                <br>
                To construct a set of vectors that are $A$ orthogonal, we would need to subract from the vector $u_i$ with components span by the vectors $d^{(i< k)}$. Mathematically:
                $$
                \begin{aligned}
                    d^{(k)} &= u_{k} + \sum_{i = 1}^{k - 1}
                        \beta_{k, i} d^{(i)}
                \end{aligned}
                \tag{9}
                $$
            </blockquote>
            <h3>Looking for $\beta_{k, i}$</h3>
            Choose $m < k$ and consider: 
            $$
                \begin{aligned}
                    d^{(m)T}Ad^{(k)} &= d^{(m)T}Au_{k} + d^{(m)T}A 
                        \sum_{i = 1}^{k - 1}\beta_{k, i}d^{(i)}
                    \\
                    0 &= d^{(m)T}Au_{k} + \beta_{k, m}d^{(m)T}Ad^{(m)}
                    \\
                    \beta_{k, m} &= 
                    - \frac
                    {
                        d^{(m)T}Au_{k}
                    }
                    {
                        \Vert d^{(m)}\Vert_A^2
                    }
                    \\
                    \implies 
                    d^{(k)} &= u_{k} - \sum_{i = 1}^{k - 1}
                    \frac{
                        d^{(i)T}Au_{k}
                    }{
                        \Vert d^{(i)}\Vert_A^2
                    }
                    d^{(i)}
                \end{aligned}
                \tag{10}
            $$
            <h4>Exercise for the Readers</h4>
            <p>Reformulate the GS Conjugation without the orthogonality assumption on $u_k$. I will be too easy on the readers if it's just me who is doing the math. Regardless of who you are, I encourage you to think about it, because you are reading it, you should think about it even if you are a recruiter who stumbled upon my websites.</p>
            <h3>The Magical Ingredient: $r^{(k)}$</h3>
            <p>
                Using the residual vector the guide the generative process of the conjugate directions will tremendously reduce the complexity of the GS conjugation, because it won't need all previous $d^{(i < k - 1)}$ vectors anymore. 
            </p>
            <h4>Claim 4</h4>
            <blockquote>
                The residual direction at the $j$ of the iteration is orthogonal to all previous conjugate direction, assuming conjugate directions for the steepest gradient descend, mathematically: 
                $$r^{(j)T}d^{(i)} = 0 \quad \forall i < j$$
            </blockquote>
            <h4>Claim 4 Proof</h4>
            <p>Recalled from Defined Quantities and Consider</p>
            $$
            \begin{aligned}
                e^{(k)} &= e^{(0)} + \sum_{j = 0}^{k - 1}\alpha_jd^{(j)}
                \\
                \underset{(4)}{\implies}
                e^{(k)}&= 
                e^{(0)} - \sum_{j = 0}^{k - 1} \delta_jd^{(j)}
                \\
                \implies
                e^{(k)}&= 
                \sum_{j = k}^{n - 1} \delta_j d^{(j)}
                \\
                \underset{\text{let: } k = j}{\implies}
                e^{(j)} &= \sum_{k = j}^{n - 1} \delta_kd^{(k)}
                \\
                Ae^{(j)} &= \sum_{k = j}^{n - 1}
                    \delta_k Ad^{(k)}
                \\
                -d^{(i)T}Ae^{(j)} &= \underbrace{-d^{(i)T}\sum_{k = j}^{n - 1}\delta_k Ad^{(k)}}_{= 0}
                \\
                d^{(i)}r^{(j)} &= 0 \quad \forall\; i < j
            \end{aligned}\tag{11}
            $$
            <p>Claim 4 is proven $\blacksquare$</p>
            <h4>Claim 5</h4>
            <blockquote>
                In addition to $r^{(k)}$ being orthogonal to all previous conjugate vectors, it's also authogonal to all previous $u_i$ that assists with the generative process of the conjugate vectors.
                $$r^{(i)}\perp u_{j} \quad \forall i < j$$
            </blockquote>
            <h4>Claim 5 proof</h4>
            $$
            \begin{aligned}
                d^{(i)} &= u_i + \sum_{k = 0}^{i - 1} \beta_{i, k}d^{(k)}
                \\
                r^{(j)T}d^{(i)} &= r^{(j)T}u_i + r^{(j)T}\sum_{k = 0}^{i - 1}\beta_{i, k}d^{(k)}
                \\
                \underset{\text{claim 4}}{\implies} 0 &= r^{(j)T}u_i
            \end{aligned}\tag{12}
            $$
            <p>Claim 5 is proven $\blacksquare$</p>
            <h4>Claim 6</h4>
            <p>
                We are getting close to the essence of the conjugate gradient algorithm. 
            </p>
            <blockquote>
                Using the residual vector during the iterations will simplify the GS Conjugation process significantly. basically let $u_i = r^{(i)}$. 
            </blockquote>
        </section>
        <section id="boxblur">
        </section>
        <div class="footer-padding">
        </div>
    </div>
    <footer>
        <!--For footer context botton-->
        <div class="fixed-bottom hide" id="footer-display">
            <div class="row align-items-center" id="footer-display-inner">
                <div class="col text-right">
                    <button type="button" class="btn btn-primary toggle-btn">Toggle Big Printout</button>
                </div>
            </div>
        </div>
        <!--Ends-->
    </footer>
</body>
</html>